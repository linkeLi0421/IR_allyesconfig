; ModuleID = '/llk/IR_all_yes/lib/siphash.c_pt.bc'
source_filename = "../lib/siphash.c"
target datalayout = "E-m:e-p:32:32-Fi8-i64:64-v128:64:128-a:0:32-n32-S64"
target triple = "armebv6k-unknown-linux-gnueabi"

module asm ".syntax unified"
module asm "\09.section \22___kcrctab+__siphash_unaligned\22, \22a\22\09"
module asm "\09.weak\09__crc___siphash_unaligned\09\09\09\09"
module asm "\09.long\09__crc___siphash_unaligned\09\09\09\09"
module asm "\09.previous\09\09\09\09\09"
module asm "\09.section \22__ksymtab_strings\22,\22aMS\22,%progbits,1\09"
module asm "__kstrtab___siphash_unaligned:\09\09\09\09\09"
module asm "\09.asciz \09\22__siphash_unaligned\22\09\09\09\09\09"
module asm "__kstrtabns___siphash_unaligned:\09\09\09\09\09"
module asm "\09.asciz \09\22\22\09\09\09\09\09"
module asm "\09.previous\09\09\09\09\09\09"
module asm "\09.section \22___kcrctab+siphash_1u64\22, \22a\22\09"
module asm "\09.weak\09__crc_siphash_1u64\09\09\09\09"
module asm "\09.long\09__crc_siphash_1u64\09\09\09\09"
module asm "\09.previous\09\09\09\09\09"
module asm "\09.section \22__ksymtab_strings\22,\22aMS\22,%progbits,1\09"
module asm "__kstrtab_siphash_1u64:\09\09\09\09\09"
module asm "\09.asciz \09\22siphash_1u64\22\09\09\09\09\09"
module asm "__kstrtabns_siphash_1u64:\09\09\09\09\09"
module asm "\09.asciz \09\22\22\09\09\09\09\09"
module asm "\09.previous\09\09\09\09\09\09"
module asm "\09.section \22___kcrctab+siphash_2u64\22, \22a\22\09"
module asm "\09.weak\09__crc_siphash_2u64\09\09\09\09"
module asm "\09.long\09__crc_siphash_2u64\09\09\09\09"
module asm "\09.previous\09\09\09\09\09"
module asm "\09.section \22__ksymtab_strings\22,\22aMS\22,%progbits,1\09"
module asm "__kstrtab_siphash_2u64:\09\09\09\09\09"
module asm "\09.asciz \09\22siphash_2u64\22\09\09\09\09\09"
module asm "__kstrtabns_siphash_2u64:\09\09\09\09\09"
module asm "\09.asciz \09\22\22\09\09\09\09\09"
module asm "\09.previous\09\09\09\09\09\09"
module asm "\09.section \22___kcrctab+siphash_3u64\22, \22a\22\09"
module asm "\09.weak\09__crc_siphash_3u64\09\09\09\09"
module asm "\09.long\09__crc_siphash_3u64\09\09\09\09"
module asm "\09.previous\09\09\09\09\09"
module asm "\09.section \22__ksymtab_strings\22,\22aMS\22,%progbits,1\09"
module asm "__kstrtab_siphash_3u64:\09\09\09\09\09"
module asm "\09.asciz \09\22siphash_3u64\22\09\09\09\09\09"
module asm "__kstrtabns_siphash_3u64:\09\09\09\09\09"
module asm "\09.asciz \09\22\22\09\09\09\09\09"
module asm "\09.previous\09\09\09\09\09\09"
module asm "\09.section \22___kcrctab+siphash_4u64\22, \22a\22\09"
module asm "\09.weak\09__crc_siphash_4u64\09\09\09\09"
module asm "\09.long\09__crc_siphash_4u64\09\09\09\09"
module asm "\09.previous\09\09\09\09\09"
module asm "\09.section \22__ksymtab_strings\22,\22aMS\22,%progbits,1\09"
module asm "__kstrtab_siphash_4u64:\09\09\09\09\09"
module asm "\09.asciz \09\22siphash_4u64\22\09\09\09\09\09"
module asm "__kstrtabns_siphash_4u64:\09\09\09\09\09"
module asm "\09.asciz \09\22\22\09\09\09\09\09"
module asm "\09.previous\09\09\09\09\09\09"
module asm "\09.section \22___kcrctab+siphash_1u32\22, \22a\22\09"
module asm "\09.weak\09__crc_siphash_1u32\09\09\09\09"
module asm "\09.long\09__crc_siphash_1u32\09\09\09\09"
module asm "\09.previous\09\09\09\09\09"
module asm "\09.section \22__ksymtab_strings\22,\22aMS\22,%progbits,1\09"
module asm "__kstrtab_siphash_1u32:\09\09\09\09\09"
module asm "\09.asciz \09\22siphash_1u32\22\09\09\09\09\09"
module asm "__kstrtabns_siphash_1u32:\09\09\09\09\09"
module asm "\09.asciz \09\22\22\09\09\09\09\09"
module asm "\09.previous\09\09\09\09\09\09"
module asm "\09.section \22___kcrctab+siphash_3u32\22, \22a\22\09"
module asm "\09.weak\09__crc_siphash_3u32\09\09\09\09"
module asm "\09.long\09__crc_siphash_3u32\09\09\09\09"
module asm "\09.previous\09\09\09\09\09"
module asm "\09.section \22__ksymtab_strings\22,\22aMS\22,%progbits,1\09"
module asm "__kstrtab_siphash_3u32:\09\09\09\09\09"
module asm "\09.asciz \09\22siphash_3u32\22\09\09\09\09\09"
module asm "__kstrtabns_siphash_3u32:\09\09\09\09\09"
module asm "\09.asciz \09\22\22\09\09\09\09\09"
module asm "\09.previous\09\09\09\09\09\09"
module asm "\09.section \22___kcrctab+__hsiphash_unaligned\22, \22a\22\09"
module asm "\09.weak\09__crc___hsiphash_unaligned\09\09\09\09"
module asm "\09.long\09__crc___hsiphash_unaligned\09\09\09\09"
module asm "\09.previous\09\09\09\09\09"
module asm "\09.section \22__ksymtab_strings\22,\22aMS\22,%progbits,1\09"
module asm "__kstrtab___hsiphash_unaligned:\09\09\09\09\09"
module asm "\09.asciz \09\22__hsiphash_unaligned\22\09\09\09\09\09"
module asm "__kstrtabns___hsiphash_unaligned:\09\09\09\09\09"
module asm "\09.asciz \09\22\22\09\09\09\09\09"
module asm "\09.previous\09\09\09\09\09\09"
module asm "\09.section \22___kcrctab+hsiphash_1u32\22, \22a\22\09"
module asm "\09.weak\09__crc_hsiphash_1u32\09\09\09\09"
module asm "\09.long\09__crc_hsiphash_1u32\09\09\09\09"
module asm "\09.previous\09\09\09\09\09"
module asm "\09.section \22__ksymtab_strings\22,\22aMS\22,%progbits,1\09"
module asm "__kstrtab_hsiphash_1u32:\09\09\09\09\09"
module asm "\09.asciz \09\22hsiphash_1u32\22\09\09\09\09\09"
module asm "__kstrtabns_hsiphash_1u32:\09\09\09\09\09"
module asm "\09.asciz \09\22\22\09\09\09\09\09"
module asm "\09.previous\09\09\09\09\09\09"
module asm "\09.section \22___kcrctab+hsiphash_2u32\22, \22a\22\09"
module asm "\09.weak\09__crc_hsiphash_2u32\09\09\09\09"
module asm "\09.long\09__crc_hsiphash_2u32\09\09\09\09"
module asm "\09.previous\09\09\09\09\09"
module asm "\09.section \22__ksymtab_strings\22,\22aMS\22,%progbits,1\09"
module asm "__kstrtab_hsiphash_2u32:\09\09\09\09\09"
module asm "\09.asciz \09\22hsiphash_2u32\22\09\09\09\09\09"
module asm "__kstrtabns_hsiphash_2u32:\09\09\09\09\09"
module asm "\09.asciz \09\22\22\09\09\09\09\09"
module asm "\09.previous\09\09\09\09\09\09"
module asm "\09.section \22___kcrctab+hsiphash_3u32\22, \22a\22\09"
module asm "\09.weak\09__crc_hsiphash_3u32\09\09\09\09"
module asm "\09.long\09__crc_hsiphash_3u32\09\09\09\09"
module asm "\09.previous\09\09\09\09\09"
module asm "\09.section \22__ksymtab_strings\22,\22aMS\22,%progbits,1\09"
module asm "__kstrtab_hsiphash_3u32:\09\09\09\09\09"
module asm "\09.asciz \09\22hsiphash_3u32\22\09\09\09\09\09"
module asm "__kstrtabns_hsiphash_3u32:\09\09\09\09\09"
module asm "\09.asciz \09\22\22\09\09\09\09\09"
module asm "\09.previous\09\09\09\09\09\09"
module asm "\09.section \22___kcrctab+hsiphash_4u32\22, \22a\22\09"
module asm "\09.weak\09__crc_hsiphash_4u32\09\09\09\09"
module asm "\09.long\09__crc_hsiphash_4u32\09\09\09\09"
module asm "\09.previous\09\09\09\09\09"
module asm "\09.section \22__ksymtab_strings\22,\22aMS\22,%progbits,1\09"
module asm "__kstrtab_hsiphash_4u32:\09\09\09\09\09"
module asm "\09.asciz \09\22hsiphash_4u32\22\09\09\09\09\09"
module asm "__kstrtabns_hsiphash_4u32:\09\09\09\09\09"
module asm "\09.asciz \09\22\22\09\09\09\09\09"
module asm "\09.previous\09\09\09\09\09\09"

%struct.kernel_symbol = type { i32, ptr, ptr }

@__kstrtab___siphash_unaligned = external dso_local constant [0 x i8], align 1
@__kstrtabns___siphash_unaligned = external dso_local constant [0 x i8], align 1
@__ksymtab___siphash_unaligned = internal constant %struct.kernel_symbol { i32 ptrtoint (ptr @__siphash_unaligned to i32), ptr @__kstrtab___siphash_unaligned, ptr @__kstrtabns___siphash_unaligned }, section "___ksymtab+__siphash_unaligned", align 4
@__kstrtab_siphash_1u64 = external dso_local constant [0 x i8], align 1
@__kstrtabns_siphash_1u64 = external dso_local constant [0 x i8], align 1
@__ksymtab_siphash_1u64 = internal constant %struct.kernel_symbol { i32 ptrtoint (ptr @siphash_1u64 to i32), ptr @__kstrtab_siphash_1u64, ptr @__kstrtabns_siphash_1u64 }, section "___ksymtab+siphash_1u64", align 4
@__kstrtab_siphash_2u64 = external dso_local constant [0 x i8], align 1
@__kstrtabns_siphash_2u64 = external dso_local constant [0 x i8], align 1
@__ksymtab_siphash_2u64 = internal constant %struct.kernel_symbol { i32 ptrtoint (ptr @siphash_2u64 to i32), ptr @__kstrtab_siphash_2u64, ptr @__kstrtabns_siphash_2u64 }, section "___ksymtab+siphash_2u64", align 4
@__kstrtab_siphash_3u64 = external dso_local constant [0 x i8], align 1
@__kstrtabns_siphash_3u64 = external dso_local constant [0 x i8], align 1
@__ksymtab_siphash_3u64 = internal constant %struct.kernel_symbol { i32 ptrtoint (ptr @siphash_3u64 to i32), ptr @__kstrtab_siphash_3u64, ptr @__kstrtabns_siphash_3u64 }, section "___ksymtab+siphash_3u64", align 4
@__kstrtab_siphash_4u64 = external dso_local constant [0 x i8], align 1
@__kstrtabns_siphash_4u64 = external dso_local constant [0 x i8], align 1
@__ksymtab_siphash_4u64 = internal constant %struct.kernel_symbol { i32 ptrtoint (ptr @siphash_4u64 to i32), ptr @__kstrtab_siphash_4u64, ptr @__kstrtabns_siphash_4u64 }, section "___ksymtab+siphash_4u64", align 4
@__kstrtab_siphash_1u32 = external dso_local constant [0 x i8], align 1
@__kstrtabns_siphash_1u32 = external dso_local constant [0 x i8], align 1
@__ksymtab_siphash_1u32 = internal constant %struct.kernel_symbol { i32 ptrtoint (ptr @siphash_1u32 to i32), ptr @__kstrtab_siphash_1u32, ptr @__kstrtabns_siphash_1u32 }, section "___ksymtab+siphash_1u32", align 4
@__kstrtab_siphash_3u32 = external dso_local constant [0 x i8], align 1
@__kstrtabns_siphash_3u32 = external dso_local constant [0 x i8], align 1
@__ksymtab_siphash_3u32 = internal constant %struct.kernel_symbol { i32 ptrtoint (ptr @siphash_3u32 to i32), ptr @__kstrtab_siphash_3u32, ptr @__kstrtabns_siphash_3u32 }, section "___ksymtab+siphash_3u32", align 4
@__kstrtab___hsiphash_unaligned = external dso_local constant [0 x i8], align 1
@__kstrtabns___hsiphash_unaligned = external dso_local constant [0 x i8], align 1
@__ksymtab___hsiphash_unaligned = internal constant %struct.kernel_symbol { i32 ptrtoint (ptr @__hsiphash_unaligned to i32), ptr @__kstrtab___hsiphash_unaligned, ptr @__kstrtabns___hsiphash_unaligned }, section "___ksymtab+__hsiphash_unaligned", align 4
@__kstrtab_hsiphash_1u32 = external dso_local constant [0 x i8], align 1
@__kstrtabns_hsiphash_1u32 = external dso_local constant [0 x i8], align 1
@__ksymtab_hsiphash_1u32 = internal constant %struct.kernel_symbol { i32 ptrtoint (ptr @hsiphash_1u32 to i32), ptr @__kstrtab_hsiphash_1u32, ptr @__kstrtabns_hsiphash_1u32 }, section "___ksymtab+hsiphash_1u32", align 4
@__kstrtab_hsiphash_2u32 = external dso_local constant [0 x i8], align 1
@__kstrtabns_hsiphash_2u32 = external dso_local constant [0 x i8], align 1
@__ksymtab_hsiphash_2u32 = internal constant %struct.kernel_symbol { i32 ptrtoint (ptr @hsiphash_2u32 to i32), ptr @__kstrtab_hsiphash_2u32, ptr @__kstrtabns_hsiphash_2u32 }, section "___ksymtab+hsiphash_2u32", align 4
@__kstrtab_hsiphash_3u32 = external dso_local constant [0 x i8], align 1
@__kstrtabns_hsiphash_3u32 = external dso_local constant [0 x i8], align 1
@__ksymtab_hsiphash_3u32 = internal constant %struct.kernel_symbol { i32 ptrtoint (ptr @hsiphash_3u32 to i32), ptr @__kstrtab_hsiphash_3u32, ptr @__kstrtabns_hsiphash_3u32 }, section "___ksymtab+hsiphash_3u32", align 4
@__kstrtab_hsiphash_4u32 = external dso_local constant [0 x i8], align 1
@__kstrtabns_hsiphash_4u32 = external dso_local constant [0 x i8], align 1
@__ksymtab_hsiphash_4u32 = internal constant %struct.kernel_symbol { i32 ptrtoint (ptr @hsiphash_4u32 to i32), ptr @__kstrtab_hsiphash_4u32, ptr @__kstrtabns_hsiphash_4u32 }, section "___ksymtab+hsiphash_4u32", align 4
@__sancov_gen_cov_switch_values = internal global [9 x i64] [i64 7, i64 32, i64 1, i64 2, i64 3, i64 4, i64 5, i64 6, i64 7]
@__sancov_gen_cov_switch_values.1 = internal global [5 x i64] [i64 3, i64 32, i64 1, i64 2, i64 3]
@llvm.compiler.used = appending global [12 x ptr] [ptr @__ksymtab___hsiphash_unaligned, ptr @__ksymtab___siphash_unaligned, ptr @__ksymtab_hsiphash_1u32, ptr @__ksymtab_hsiphash_2u32, ptr @__ksymtab_hsiphash_3u32, ptr @__ksymtab_hsiphash_4u32, ptr @__ksymtab_siphash_1u32, ptr @__ksymtab_siphash_1u64, ptr @__ksymtab_siphash_2u64, ptr @__ksymtab_siphash_3u32, ptr @__ksymtab_siphash_3u64, ptr @__ksymtab_siphash_4u64], section "llvm.metadata"
@llvm.used = appending global [1 x ptr] [ptr @asan.module_ctor], section "llvm.metadata"
@llvm.global_ctors = appending global [1 x { i32, ptr, ptr }] [{ i32, ptr, ptr } { i32 1, ptr @asan.module_ctor, ptr null }]

; Function Attrs: nofree nosync nounwind null_pointer_is_valid readonly sanitize_address sspstrong uwtable(sync)
define dso_local i64 @__siphash_unaligned(ptr noundef readonly %data, i32 noundef %len, ptr nocapture noundef readonly %key) #0 align 64 {
entry:
  call void @__sanitizer_cov_trace_pc() #4
  %add.ptr = getelementptr i8, ptr %data, i32 %len
  %rem = and i32 %len, 7
  %idx.neg = sub nsw i32 0, %rem
  %add.ptr1 = getelementptr i8, ptr %add.ptr, i32 %idx.neg
  %conv2 = zext i32 %len to i64
  %shl = shl i64 %conv2, 56
  %arrayidx = getelementptr [2 x i64], ptr %key, i32 0, i32 1
  %0 = ptrtoint ptr %arrayidx to i32
  call void @__asan_load8_noabort(i32 %0)
  %1 = load i64, ptr %arrayidx, align 8
  %xor = xor i64 %1, 8387220255154660723
  %2 = ptrtoint ptr %key to i32
  call void @__asan_load8_noabort(i32 %2)
  %3 = load i64, ptr %key, align 8
  %xor6 = xor i64 %3, 7816392313619706465
  %xor9 = xor i64 %1, 7237128888997146477
  %xor12 = xor i64 %3, 8317987319222330741
  %cmp.not443 = icmp eq ptr %add.ptr1, %data
  br i1 %cmp.not443, label %entry.for.end_crit_edge, label %entry.for.body_crit_edge

entry.for.body_crit_edge:                         ; preds = %entry
  br label %for.body

entry.for.end_crit_edge:                          ; preds = %entry
  call void @__sanitizer_cov_trace_pc() #4
  br label %for.end

for.body:                                         ; preds = %for.body.for.body_crit_edge, %entry.for.body_crit_edge
  %v3.0448 = phi i64 [ %xor38, %for.body.for.body_crit_edge ], [ %xor, %entry.for.body_crit_edge ]
  %v2.0447 = phi i64 [ %or.i406, %for.body.for.body_crit_edge ], [ %xor6, %entry.for.body_crit_edge ]
  %v1.0446 = phi i64 [ %xor41, %for.body.for.body_crit_edge ], [ %xor9, %entry.for.body_crit_edge ]
  %v0.0445 = phi i64 [ %xor45, %for.body.for.body_crit_edge ], [ %xor12, %entry.for.body_crit_edge ]
  %data.addr.0444 = phi ptr [ %add.ptr46, %for.body.for.body_crit_edge ], [ %data, %entry.for.body_crit_edge ]
  %4 = ptrtoint ptr %data.addr.0444 to i32
  call void @__asan_loadN_noabort(i32 %4, i32 8)
  %5 = load i64, ptr %data.addr.0444, align 1
  %6 = tail call i64 @llvm.bswap.i64(i64 %5) #5
  %xor14 = xor i64 %6, %v3.0448
  %add = add i64 %v1.0446, %v0.0445
  %or.i = tail call i64 @llvm.fshl.i64(i64 %v1.0446, i64 %v1.0446, i64 13) #5
  %xor16 = xor i64 %or.i, %add
  %or.i396 = tail call i64 @llvm.fshl.i64(i64 %add, i64 %add, i64 32) #5
  %add18 = add i64 %xor14, %v2.0447
  %or.i397 = tail call i64 @llvm.fshl.i64(i64 %xor14, i64 %xor14, i64 16) #5
  %xor20 = xor i64 %or.i397, %add18
  %add21 = add i64 %xor20, %or.i396
  %or.i398 = tail call i64 @llvm.fshl.i64(i64 %xor20, i64 %xor20, i64 21) #5
  %xor23 = xor i64 %or.i398, %add21
  %add24 = add i64 %add18, %xor16
  %or.i399 = tail call i64 @llvm.fshl.i64(i64 %xor16, i64 %xor16, i64 17) #5
  %xor26 = xor i64 %add24, %or.i399
  %or.i400 = tail call i64 @llvm.fshl.i64(i64 %add24, i64 %add24, i64 32) #5
  %add29 = add i64 %add21, %xor26
  %or.i401 = tail call i64 @llvm.fshl.i64(i64 %xor26, i64 %xor26, i64 13) #5
  %xor31 = xor i64 %or.i401, %add29
  %or.i402 = tail call i64 @llvm.fshl.i64(i64 %add29, i64 %add29, i64 32) #5
  %add33 = add i64 %xor23, %or.i400
  %or.i403 = tail call i64 @llvm.fshl.i64(i64 %xor23, i64 %xor23, i64 16) #5
  %xor35 = xor i64 %or.i403, %add33
  %add36 = add i64 %xor35, %or.i402
  %or.i404 = tail call i64 @llvm.fshl.i64(i64 %xor35, i64 %xor35, i64 21) #5
  %xor38 = xor i64 %or.i404, %add36
  %add39 = add i64 %add33, %xor31
  %or.i405 = tail call i64 @llvm.fshl.i64(i64 %xor31, i64 %xor31, i64 17) #5
  %xor41 = xor i64 %or.i405, %add39
  %or.i406 = tail call i64 @llvm.fshl.i64(i64 %add39, i64 %add39, i64 32) #5
  %xor45 = xor i64 %add36, %6
  %add.ptr46 = getelementptr i8, ptr %data.addr.0444, i32 8
  %cmp.not = icmp eq ptr %add.ptr46, %add.ptr1
  br i1 %cmp.not, label %for.body.for.end_crit_edge, label %for.body.for.body_crit_edge

for.body.for.body_crit_edge:                      ; preds = %for.body
  call void @__sanitizer_cov_trace_pc() #4
  br label %for.body

for.body.for.end_crit_edge:                       ; preds = %for.body
  call void @__sanitizer_cov_trace_pc() #4
  br label %for.end

for.end:                                          ; preds = %for.body.for.end_crit_edge, %entry.for.end_crit_edge
  %v0.0.lcssa = phi i64 [ %xor12, %entry.for.end_crit_edge ], [ %xor45, %for.body.for.end_crit_edge ]
  %v1.0.lcssa = phi i64 [ %xor9, %entry.for.end_crit_edge ], [ %xor41, %for.body.for.end_crit_edge ]
  %v2.0.lcssa = phi i64 [ %xor6, %entry.for.end_crit_edge ], [ %or.i406, %for.body.for.end_crit_edge ]
  %v3.0.lcssa = phi i64 [ %xor, %entry.for.end_crit_edge ], [ %xor38, %for.body.for.end_crit_edge ]
  %7 = zext i32 %rem to i64
  call void @__sanitizer_cov_trace_switch(i64 %7, ptr @__sancov_gen_cov_switch_values)
  switch i32 %rem, label %for.end.sw.epilog_crit_edge [
    i32 7, label %sw.bb
    i32 6, label %for.end.sw.bb51_crit_edge
    i32 5, label %for.end.sw.bb56_crit_edge
    i32 4, label %for.end.sw.bb61_crit_edge
    i32 3, label %sw.bb65
    i32 2, label %for.end.sw.bb70_crit_edge
    i32 1, label %sw.bb74
  ]

for.end.sw.bb70_crit_edge:                        ; preds = %for.end
  call void @__sanitizer_cov_trace_pc() #4
  br label %sw.bb70

for.end.sw.bb61_crit_edge:                        ; preds = %for.end
  call void @__sanitizer_cov_trace_pc() #4
  br label %sw.bb61

for.end.sw.bb56_crit_edge:                        ; preds = %for.end
  call void @__sanitizer_cov_trace_pc() #4
  br label %sw.bb56

for.end.sw.bb51_crit_edge:                        ; preds = %for.end
  call void @__sanitizer_cov_trace_pc() #4
  br label %sw.bb51

for.end.sw.epilog_crit_edge:                      ; preds = %for.end
  call void @__sanitizer_cov_trace_pc() #4
  br label %sw.epilog

sw.bb:                                            ; preds = %for.end
  call void @__sanitizer_cov_trace_pc() #4
  %arrayidx48 = getelementptr i8, ptr %add.ptr1, i32 6
  %8 = ptrtoint ptr %arrayidx48 to i32
  call void @__asan_load1_noabort(i32 %8)
  %9 = load i8, ptr %arrayidx48, align 1
  %conv49 = zext i8 %9 to i64
  %shl50 = shl nuw nsw i64 %conv49, 48
  %or = or i64 %shl50, %shl
  br label %sw.bb51

sw.bb51:                                          ; preds = %sw.bb, %for.end.sw.bb51_crit_edge
  %b.0 = phi i64 [ %shl, %for.end.sw.bb51_crit_edge ], [ %or, %sw.bb ]
  %arrayidx52 = getelementptr i8, ptr %add.ptr1, i32 5
  %10 = ptrtoint ptr %arrayidx52 to i32
  call void @__asan_load1_noabort(i32 %10)
  %11 = load i8, ptr %arrayidx52, align 1
  %conv53 = zext i8 %11 to i64
  %shl54 = shl nuw nsw i64 %conv53, 40
  %or55 = or i64 %shl54, %b.0
  br label %sw.bb56

sw.bb56:                                          ; preds = %sw.bb51, %for.end.sw.bb56_crit_edge
  %b.1 = phi i64 [ %shl, %for.end.sw.bb56_crit_edge ], [ %or55, %sw.bb51 ]
  %arrayidx57 = getelementptr i8, ptr %add.ptr1, i32 4
  %12 = ptrtoint ptr %arrayidx57 to i32
  call void @__asan_load1_noabort(i32 %12)
  %13 = load i8, ptr %arrayidx57, align 1
  %conv58 = zext i8 %13 to i64
  %shl59 = shl nuw nsw i64 %conv58, 32
  %or60 = or i64 %shl59, %b.1
  br label %sw.bb61

sw.bb61:                                          ; preds = %sw.bb56, %for.end.sw.bb61_crit_edge
  %b.2 = phi i64 [ %shl, %for.end.sw.bb61_crit_edge ], [ %or60, %sw.bb56 ]
  %14 = ptrtoint ptr %add.ptr1 to i32
  call void @__asan_loadN_noabort(i32 %14, i32 4)
  %15 = load i32, ptr %add.ptr1, align 1
  %16 = tail call i32 @llvm.bswap.i32(i32 %15) #5
  %conv63 = zext i32 %16 to i64
  %or64 = or i64 %b.2, %conv63
  br label %sw.epilog

sw.bb65:                                          ; preds = %for.end
  call void @__sanitizer_cov_trace_pc() #4
  %arrayidx66 = getelementptr i8, ptr %add.ptr1, i32 2
  %17 = ptrtoint ptr %arrayidx66 to i32
  call void @__asan_load1_noabort(i32 %17)
  %18 = load i8, ptr %arrayidx66, align 1
  %conv67 = zext i8 %18 to i64
  %shl68 = shl nuw nsw i64 %conv67, 16
  %or69 = or i64 %shl68, %shl
  br label %sw.bb70

sw.bb70:                                          ; preds = %sw.bb65, %for.end.sw.bb70_crit_edge
  %b.3 = phi i64 [ %shl, %for.end.sw.bb70_crit_edge ], [ %or69, %sw.bb65 ]
  %19 = ptrtoint ptr %add.ptr1 to i32
  call void @__asan_loadN_noabort(i32 %19, i32 2)
  %20 = load i16, ptr %add.ptr1, align 1
  %21 = tail call i16 @llvm.bswap.i16(i16 %20) #5
  %conv72 = zext i16 %21 to i64
  %or73 = or i64 %b.3, %conv72
  br label %sw.epilog

sw.bb74:                                          ; preds = %for.end
  call void @__sanitizer_cov_trace_pc() #4
  %22 = ptrtoint ptr %add.ptr1 to i32
  call void @__asan_load1_noabort(i32 %22)
  %23 = load i8, ptr %add.ptr1, align 1
  %conv76 = zext i8 %23 to i64
  %or77 = or i64 %shl, %conv76
  br label %sw.epilog

sw.epilog:                                        ; preds = %sw.bb74, %sw.bb70, %sw.bb61, %for.end.sw.epilog_crit_edge
  %b.4 = phi i64 [ %shl, %for.end.sw.epilog_crit_edge ], [ %or77, %sw.bb74 ], [ %or73, %sw.bb70 ], [ %or64, %sw.bb61 ]
  %xor78 = xor i64 %b.4, %v3.0.lcssa
  %add80 = add i64 %v1.0.lcssa, %v0.0.lcssa
  %or.i407 = tail call i64 @llvm.fshl.i64(i64 %v1.0.lcssa, i64 %v1.0.lcssa, i64 13) #5
  %xor82 = xor i64 %or.i407, %add80
  %or.i408 = tail call i64 @llvm.fshl.i64(i64 %add80, i64 %add80, i64 32) #5
  %add84 = add i64 %xor78, %v2.0.lcssa
  %or.i409 = tail call i64 @llvm.fshl.i64(i64 %xor78, i64 %xor78, i64 16) #5
  %xor86 = xor i64 %or.i409, %add84
  %add87 = add i64 %xor86, %or.i408
  %or.i410 = tail call i64 @llvm.fshl.i64(i64 %xor86, i64 %xor86, i64 21) #5
  %xor89 = xor i64 %or.i410, %add87
  %add90 = add i64 %add84, %xor82
  %or.i411 = tail call i64 @llvm.fshl.i64(i64 %xor82, i64 %xor82, i64 17) #5
  %xor92 = xor i64 %add90, %or.i411
  %or.i412 = tail call i64 @llvm.fshl.i64(i64 %add90, i64 %add90, i64 32) #5
  %add97 = add i64 %add87, %xor92
  %or.i413 = tail call i64 @llvm.fshl.i64(i64 %xor92, i64 %xor92, i64 13) #5
  %xor99 = xor i64 %or.i413, %add97
  %or.i414 = tail call i64 @llvm.fshl.i64(i64 %add97, i64 %add97, i64 32) #5
  %add101 = add i64 %xor89, %or.i412
  %or.i415 = tail call i64 @llvm.fshl.i64(i64 %xor89, i64 %xor89, i64 16) #5
  %xor103 = xor i64 %or.i415, %add101
  %add104 = add i64 %xor103, %or.i414
  %or.i416 = tail call i64 @llvm.fshl.i64(i64 %xor103, i64 %xor103, i64 21) #5
  %xor106 = xor i64 %or.i416, %add104
  %add107 = add i64 %add101, %xor99
  %or.i417 = tail call i64 @llvm.fshl.i64(i64 %xor99, i64 %xor99, i64 17) #5
  %xor109 = xor i64 %or.i417, %add107
  %or.i418 = tail call i64 @llvm.fshl.i64(i64 %add107, i64 %add107, i64 32) #5
  %xor113 = xor i64 %add104, %b.4
  %xor114 = xor i64 %or.i418, 255
  %add116 = add i64 %xor113, %xor109
  %or.i419 = tail call i64 @llvm.fshl.i64(i64 %xor109, i64 %xor109, i64 13) #5
  %xor118 = xor i64 %add116, %or.i419
  %or.i420 = tail call i64 @llvm.fshl.i64(i64 %add116, i64 %add116, i64 32) #5
  %add120 = add i64 %xor114, %xor106
  %or.i421 = tail call i64 @llvm.fshl.i64(i64 %xor106, i64 %xor106, i64 16) #5
  %xor122 = xor i64 %or.i421, %add120
  %add123 = add i64 %or.i420, %xor122
  %or.i422 = tail call i64 @llvm.fshl.i64(i64 %xor122, i64 %xor122, i64 21) #5
  %xor125 = xor i64 %or.i422, %add123
  %add126 = add i64 %xor118, %add120
  %or.i423 = tail call i64 @llvm.fshl.i64(i64 %xor118, i64 %xor118, i64 17) #5
  %xor128 = xor i64 %or.i423, %add126
  %or.i424 = tail call i64 @llvm.fshl.i64(i64 %add126, i64 %add126, i64 32) #5
  %add133 = add i64 %xor128, %add123
  %or.i425 = tail call i64 @llvm.fshl.i64(i64 %xor128, i64 %xor128, i64 13) #5
  %xor135 = xor i64 %or.i425, %add133
  %or.i426 = tail call i64 @llvm.fshl.i64(i64 %add133, i64 %add133, i64 32) #5
  %add137 = add i64 %or.i424, %xor125
  %or.i427 = tail call i64 @llvm.fshl.i64(i64 %xor125, i64 %xor125, i64 16) #5
  %xor139 = xor i64 %or.i427, %add137
  %add140 = add i64 %or.i426, %xor139
  %or.i428 = tail call i64 @llvm.fshl.i64(i64 %xor139, i64 %xor139, i64 21) #5
  %xor142 = xor i64 %or.i428, %add140
  %add143 = add i64 %xor135, %add137
  %or.i429 = tail call i64 @llvm.fshl.i64(i64 %xor135, i64 %xor135, i64 17) #5
  %xor145 = xor i64 %or.i429, %add143
  %or.i430 = tail call i64 @llvm.fshl.i64(i64 %add143, i64 %add143, i64 32) #5
  %add150 = add i64 %xor145, %add140
  %or.i431 = tail call i64 @llvm.fshl.i64(i64 %xor145, i64 %xor145, i64 13) #5
  %xor152 = xor i64 %or.i431, %add150
  %or.i432 = tail call i64 @llvm.fshl.i64(i64 %add150, i64 %add150, i64 32) #5
  %add154 = add i64 %or.i430, %xor142
  %or.i433 = tail call i64 @llvm.fshl.i64(i64 %xor142, i64 %xor142, i64 16) #5
  %xor156 = xor i64 %or.i433, %add154
  %add157 = add i64 %or.i432, %xor156
  %or.i434 = tail call i64 @llvm.fshl.i64(i64 %xor156, i64 %xor156, i64 21) #5
  %xor159 = xor i64 %or.i434, %add157
  %add160 = add i64 %xor152, %add154
  %or.i435 = tail call i64 @llvm.fshl.i64(i64 %xor152, i64 %xor152, i64 17) #5
  %xor162 = xor i64 %or.i435, %add160
  %or.i436 = tail call i64 @llvm.fshl.i64(i64 %add160, i64 %add160, i64 32) #5
  %add167 = add i64 %xor162, %add157
  %or.i437 = tail call i64 @llvm.fshl.i64(i64 %xor162, i64 %xor162, i64 13) #5
  %xor169 = xor i64 %or.i437, %add167
  %add171 = add i64 %or.i436, %xor159
  %or.i439 = tail call i64 @llvm.fshl.i64(i64 %xor159, i64 %xor159, i64 16) #5
  %xor173 = xor i64 %or.i439, %add171
  %or.i440 = tail call i64 @llvm.fshl.i64(i64 %xor173, i64 %xor173, i64 21) #5
  %add177 = add i64 %xor169, %add171
  %or.i441 = tail call i64 @llvm.fshl.i64(i64 %xor169, i64 %xor169, i64 17) #5
  %xor179 = xor i64 %or.i441, %add177
  %or.i442 = tail call i64 @llvm.fshl.i64(i64 %add177, i64 %add177, i64 32) #5
  %xor183 = xor i64 %xor179, %or.i440
  %xor185 = xor i64 %xor183, %or.i442
  ret i64 %xor185
}

; Function Attrs: argmemonly mustprogress nofree nosync nounwind null_pointer_is_valid readonly sanitize_address sspstrong willreturn uwtable(sync)
define dso_local i64 @siphash_1u64(i64 noundef %first, ptr nocapture noundef readonly %key) #1 align 64 {
entry:
  call void @__sanitizer_cov_trace_pc() #4
  %arrayidx = getelementptr [2 x i64], ptr %key, i32 0, i32 1
  %0 = ptrtoint ptr %arrayidx to i32
  call void @__asan_load8_noabort(i32 %0)
  %1 = load i64, ptr %arrayidx, align 8
  %2 = ptrtoint ptr %key to i32
  call void @__asan_load8_noabort(i32 %2)
  %3 = load i64, ptr %key, align 8
  %xor4 = xor i64 %3, 7816392313619706465
  %xor7 = xor i64 %1, 7237128888997146477
  %xor10 = xor i64 %3, 8317987319222330741
  %4 = xor i64 %1, %first
  %xor11 = xor i64 %4, 8387220255154660723
  %add = add i64 %xor10, %xor7
  %or.i = tail call i64 @llvm.fshl.i64(i64 %xor7, i64 %xor7, i64 13) #5
  %xor12 = xor i64 %add, %or.i
  %or.i340 = tail call i64 @llvm.fshl.i64(i64 %add, i64 %add, i64 32) #5
  %add14 = add i64 %xor4, %xor11
  %or.i341 = tail call i64 @llvm.fshl.i64(i64 %xor11, i64 %xor11, i64 16) #5
  %xor16 = xor i64 %add14, %or.i341
  %add17 = add i64 %or.i340, %xor16
  %or.i342 = tail call i64 @llvm.fshl.i64(i64 %xor16, i64 %xor16, i64 21) #5
  %xor19 = xor i64 %or.i342, %add17
  %add20 = add i64 %xor12, %add14
  %or.i343 = tail call i64 @llvm.fshl.i64(i64 %xor12, i64 %xor12, i64 17) #5
  %xor22 = xor i64 %or.i343, %add20
  %or.i344 = tail call i64 @llvm.fshl.i64(i64 %add20, i64 %add20, i64 32) #5
  %add25 = add i64 %xor22, %add17
  %or.i345 = tail call i64 @llvm.fshl.i64(i64 %xor22, i64 %xor22, i64 13) #5
  %xor27 = xor i64 %or.i345, %add25
  %or.i346 = tail call i64 @llvm.fshl.i64(i64 %add25, i64 %add25, i64 32) #5
  %add29 = add i64 %or.i344, %xor19
  %or.i347 = tail call i64 @llvm.fshl.i64(i64 %xor19, i64 %xor19, i64 16) #5
  %xor31 = xor i64 %or.i347, %add29
  %add32 = add i64 %or.i346, %xor31
  %or.i348 = tail call i64 @llvm.fshl.i64(i64 %xor31, i64 %xor31, i64 21) #5
  %xor34 = xor i64 %or.i348, %add32
  %add35 = add i64 %xor27, %add29
  %or.i349 = tail call i64 @llvm.fshl.i64(i64 %xor27, i64 %xor27, i64 17) #5
  %xor37 = xor i64 %or.i349, %add35
  %or.i350 = tail call i64 @llvm.fshl.i64(i64 %add35, i64 %add35, i64 32) #5
  %xor41 = xor i64 %add32, %first
  %xor42 = xor i64 %xor34, 576460752303423488
  %add44 = add i64 %xor41, %xor37
  %or.i351 = tail call i64 @llvm.fshl.i64(i64 %xor37, i64 %xor37, i64 13) #5
  %xor46 = xor i64 %or.i351, %add44
  %or.i352 = tail call i64 @llvm.fshl.i64(i64 %add44, i64 %add44, i64 32) #5
  %add48 = add i64 %xor42, %or.i350
  %or.i353 = tail call i64 @llvm.fshl.i64(i64 %xor34, i64 %xor42, i64 16) #5
  %xor50 = xor i64 %or.i353, %add48
  %add51 = add i64 %xor50, %or.i352
  %or.i354 = tail call i64 @llvm.fshl.i64(i64 %xor50, i64 %xor50, i64 21) #5
  %xor53 = xor i64 %or.i354, %add51
  %add54 = add i64 %add48, %xor46
  %or.i355 = tail call i64 @llvm.fshl.i64(i64 %xor46, i64 %xor46, i64 17) #5
  %xor56 = xor i64 %or.i355, %add54
  %or.i356 = tail call i64 @llvm.fshl.i64(i64 %add54, i64 %add54, i64 32) #5
  %add61 = add i64 %add51, %xor56
  %or.i357 = tail call i64 @llvm.fshl.i64(i64 %xor56, i64 %xor56, i64 13) #5
  %xor63 = xor i64 %or.i357, %add61
  %or.i358 = tail call i64 @llvm.fshl.i64(i64 %add61, i64 %add61, i64 32) #5
  %add65 = add i64 %xor53, %or.i356
  %or.i359 = tail call i64 @llvm.fshl.i64(i64 %xor53, i64 %xor53, i64 16) #5
  %xor67 = xor i64 %or.i359, %add65
  %add68 = add i64 %xor67, %or.i358
  %or.i360 = tail call i64 @llvm.fshl.i64(i64 %xor67, i64 %xor67, i64 21) #5
  %xor70 = xor i64 %or.i360, %add68
  %add71 = add i64 %add65, %xor63
  %or.i361 = tail call i64 @llvm.fshl.i64(i64 %xor63, i64 %xor63, i64 17) #5
  %xor73 = xor i64 %or.i361, %add71
  %or.i362 = tail call i64 @llvm.fshl.i64(i64 %add71, i64 %add71, i64 32) #5
  %xor77 = xor i64 %add68, 576460752303423488
  %xor78 = xor i64 %or.i362, 255
  %add80 = add i64 %xor77, %xor73
  %or.i363 = tail call i64 @llvm.fshl.i64(i64 %xor73, i64 %xor73, i64 13) #5
  %xor82 = xor i64 %add80, %or.i363
  %or.i364 = tail call i64 @llvm.fshl.i64(i64 %add80, i64 %add80, i64 32) #5
  %add84 = add i64 %xor78, %xor70
  %or.i365 = tail call i64 @llvm.fshl.i64(i64 %xor70, i64 %xor70, i64 16) #5
  %xor86 = xor i64 %or.i365, %add84
  %add87 = add i64 %or.i364, %xor86
  %or.i366 = tail call i64 @llvm.fshl.i64(i64 %xor86, i64 %xor86, i64 21) #5
  %xor89 = xor i64 %or.i366, %add87
  %add90 = add i64 %xor82, %add84
  %or.i367 = tail call i64 @llvm.fshl.i64(i64 %xor82, i64 %xor82, i64 17) #5
  %xor92 = xor i64 %or.i367, %add90
  %or.i368 = tail call i64 @llvm.fshl.i64(i64 %add90, i64 %add90, i64 32) #5
  %add97 = add i64 %xor92, %add87
  %or.i369 = tail call i64 @llvm.fshl.i64(i64 %xor92, i64 %xor92, i64 13) #5
  %xor99 = xor i64 %or.i369, %add97
  %or.i370 = tail call i64 @llvm.fshl.i64(i64 %add97, i64 %add97, i64 32) #5
  %add101 = add i64 %or.i368, %xor89
  %or.i371 = tail call i64 @llvm.fshl.i64(i64 %xor89, i64 %xor89, i64 16) #5
  %xor103 = xor i64 %or.i371, %add101
  %add104 = add i64 %or.i370, %xor103
  %or.i372 = tail call i64 @llvm.fshl.i64(i64 %xor103, i64 %xor103, i64 21) #5
  %xor106 = xor i64 %or.i372, %add104
  %add107 = add i64 %xor99, %add101
  %or.i373 = tail call i64 @llvm.fshl.i64(i64 %xor99, i64 %xor99, i64 17) #5
  %xor109 = xor i64 %or.i373, %add107
  %or.i374 = tail call i64 @llvm.fshl.i64(i64 %add107, i64 %add107, i64 32) #5
  %add114 = add i64 %xor109, %add104
  %or.i375 = tail call i64 @llvm.fshl.i64(i64 %xor109, i64 %xor109, i64 13) #5
  %xor116 = xor i64 %or.i375, %add114
  %or.i376 = tail call i64 @llvm.fshl.i64(i64 %add114, i64 %add114, i64 32) #5
  %add118 = add i64 %or.i374, %xor106
  %or.i377 = tail call i64 @llvm.fshl.i64(i64 %xor106, i64 %xor106, i64 16) #5
  %xor120 = xor i64 %or.i377, %add118
  %add121 = add i64 %or.i376, %xor120
  %or.i378 = tail call i64 @llvm.fshl.i64(i64 %xor120, i64 %xor120, i64 21) #5
  %xor123 = xor i64 %or.i378, %add121
  %add124 = add i64 %xor116, %add118
  %or.i379 = tail call i64 @llvm.fshl.i64(i64 %xor116, i64 %xor116, i64 17) #5
  %xor126 = xor i64 %or.i379, %add124
  %or.i380 = tail call i64 @llvm.fshl.i64(i64 %add124, i64 %add124, i64 32) #5
  %add131 = add i64 %xor126, %add121
  %or.i381 = tail call i64 @llvm.fshl.i64(i64 %xor126, i64 %xor126, i64 13) #5
  %xor133 = xor i64 %or.i381, %add131
  %add135 = add i64 %or.i380, %xor123
  %or.i383 = tail call i64 @llvm.fshl.i64(i64 %xor123, i64 %xor123, i64 16) #5
  %xor137 = xor i64 %or.i383, %add135
  %or.i384 = tail call i64 @llvm.fshl.i64(i64 %xor137, i64 %xor137, i64 21) #5
  %add141 = add i64 %xor133, %add135
  %or.i385 = tail call i64 @llvm.fshl.i64(i64 %xor133, i64 %xor133, i64 17) #5
  %xor143 = xor i64 %or.i385, %add141
  %or.i386 = tail call i64 @llvm.fshl.i64(i64 %add141, i64 %add141, i64 32) #5
  %xor147 = xor i64 %xor143, %or.i384
  %xor149 = xor i64 %xor147, %or.i386
  ret i64 %xor149
}

; Function Attrs: argmemonly mustprogress nofree nosync nounwind null_pointer_is_valid readonly sanitize_address sspstrong willreturn uwtable(sync)
define dso_local i64 @siphash_2u64(i64 noundef %first, i64 noundef %second, ptr nocapture noundef readonly %key) #1 align 64 {
entry:
  call void @__sanitizer_cov_trace_pc() #4
  %arrayidx = getelementptr [2 x i64], ptr %key, i32 0, i32 1
  %0 = ptrtoint ptr %arrayidx to i32
  call void @__asan_load8_noabort(i32 %0)
  %1 = load i64, ptr %arrayidx, align 8
  %2 = ptrtoint ptr %key to i32
  call void @__asan_load8_noabort(i32 %2)
  %3 = load i64, ptr %key, align 8
  %xor4 = xor i64 %3, 7816392313619706465
  %xor7 = xor i64 %1, 7237128888997146477
  %xor10 = xor i64 %3, 8317987319222330741
  %4 = xor i64 %1, %first
  %xor11 = xor i64 %4, 8387220255154660723
  %add = add i64 %xor10, %xor7
  %or.i = tail call i64 @llvm.fshl.i64(i64 %xor7, i64 %xor7, i64 13) #5
  %xor12 = xor i64 %add, %or.i
  %or.i423 = tail call i64 @llvm.fshl.i64(i64 %add, i64 %add, i64 32) #5
  %add14 = add i64 %xor4, %xor11
  %or.i424 = tail call i64 @llvm.fshl.i64(i64 %xor11, i64 %xor11, i64 16) #5
  %xor16 = xor i64 %add14, %or.i424
  %add17 = add i64 %or.i423, %xor16
  %or.i425 = tail call i64 @llvm.fshl.i64(i64 %xor16, i64 %xor16, i64 21) #5
  %xor19 = xor i64 %or.i425, %add17
  %add20 = add i64 %xor12, %add14
  %or.i426 = tail call i64 @llvm.fshl.i64(i64 %xor12, i64 %xor12, i64 17) #5
  %xor22 = xor i64 %or.i426, %add20
  %or.i427 = tail call i64 @llvm.fshl.i64(i64 %add20, i64 %add20, i64 32) #5
  %add25 = add i64 %xor22, %add17
  %or.i428 = tail call i64 @llvm.fshl.i64(i64 %xor22, i64 %xor22, i64 13) #5
  %xor27 = xor i64 %or.i428, %add25
  %or.i429 = tail call i64 @llvm.fshl.i64(i64 %add25, i64 %add25, i64 32) #5
  %add29 = add i64 %or.i427, %xor19
  %or.i430 = tail call i64 @llvm.fshl.i64(i64 %xor19, i64 %xor19, i64 16) #5
  %xor31 = xor i64 %or.i430, %add29
  %add32 = add i64 %or.i429, %xor31
  %or.i431 = tail call i64 @llvm.fshl.i64(i64 %xor31, i64 %xor31, i64 21) #5
  %add35 = add i64 %xor27, %add29
  %or.i432 = tail call i64 @llvm.fshl.i64(i64 %xor27, i64 %xor27, i64 17) #5
  %xor37 = xor i64 %or.i432, %add35
  %or.i433 = tail call i64 @llvm.fshl.i64(i64 %add35, i64 %add35, i64 32) #5
  %xor41 = xor i64 %add32, %first
  %xor34 = xor i64 %add32, %second
  %xor42 = xor i64 %xor34, %or.i431
  %add44 = add i64 %xor41, %xor37
  %or.i434 = tail call i64 @llvm.fshl.i64(i64 %xor37, i64 %xor37, i64 13) #5
  %xor46 = xor i64 %or.i434, %add44
  %or.i435 = tail call i64 @llvm.fshl.i64(i64 %add44, i64 %add44, i64 32) #5
  %add48 = add i64 %xor42, %or.i433
  %or.i436 = tail call i64 @llvm.fshl.i64(i64 %xor42, i64 %xor42, i64 16) #5
  %xor50 = xor i64 %or.i436, %add48
  %add51 = add i64 %xor50, %or.i435
  %or.i437 = tail call i64 @llvm.fshl.i64(i64 %xor50, i64 %xor50, i64 21) #5
  %xor53 = xor i64 %or.i437, %add51
  %add54 = add i64 %add48, %xor46
  %or.i438 = tail call i64 @llvm.fshl.i64(i64 %xor46, i64 %xor46, i64 17) #5
  %xor56 = xor i64 %or.i438, %add54
  %or.i439 = tail call i64 @llvm.fshl.i64(i64 %add54, i64 %add54, i64 32) #5
  %add61 = add i64 %add51, %xor56
  %or.i440 = tail call i64 @llvm.fshl.i64(i64 %xor56, i64 %xor56, i64 13) #5
  %xor63 = xor i64 %or.i440, %add61
  %or.i441 = tail call i64 @llvm.fshl.i64(i64 %add61, i64 %add61, i64 32) #5
  %add65 = add i64 %xor53, %or.i439
  %or.i442 = tail call i64 @llvm.fshl.i64(i64 %xor53, i64 %xor53, i64 16) #5
  %xor67 = xor i64 %or.i442, %add65
  %add68 = add i64 %xor67, %or.i441
  %or.i443 = tail call i64 @llvm.fshl.i64(i64 %xor67, i64 %xor67, i64 21) #5
  %xor70 = xor i64 %or.i443, %add68
  %add71 = add i64 %add65, %xor63
  %or.i444 = tail call i64 @llvm.fshl.i64(i64 %xor63, i64 %xor63, i64 17) #5
  %xor73 = xor i64 %or.i444, %add71
  %or.i445 = tail call i64 @llvm.fshl.i64(i64 %add71, i64 %add71, i64 32) #5
  %xor77 = xor i64 %add68, %second
  %xor78 = xor i64 %xor70, 1152921504606846976
  %add80 = add i64 %xor77, %xor73
  %or.i446 = tail call i64 @llvm.fshl.i64(i64 %xor73, i64 %xor73, i64 13) #5
  %xor82 = xor i64 %add80, %or.i446
  %or.i447 = tail call i64 @llvm.fshl.i64(i64 %add80, i64 %add80, i64 32) #5
  %add84 = add i64 %xor78, %or.i445
  %or.i448 = tail call i64 @llvm.fshl.i64(i64 %xor70, i64 %xor78, i64 16) #5
  %xor86 = xor i64 %or.i448, %add84
  %add87 = add i64 %xor86, %or.i447
  %or.i449 = tail call i64 @llvm.fshl.i64(i64 %xor86, i64 %xor86, i64 21) #5
  %xor89 = xor i64 %or.i449, %add87
  %add90 = add i64 %add84, %xor82
  %or.i450 = tail call i64 @llvm.fshl.i64(i64 %xor82, i64 %xor82, i64 17) #5
  %xor92 = xor i64 %or.i450, %add90
  %or.i451 = tail call i64 @llvm.fshl.i64(i64 %add90, i64 %add90, i64 32) #5
  %add97 = add i64 %add87, %xor92
  %or.i452 = tail call i64 @llvm.fshl.i64(i64 %xor92, i64 %xor92, i64 13) #5
  %xor99 = xor i64 %or.i452, %add97
  %or.i453 = tail call i64 @llvm.fshl.i64(i64 %add97, i64 %add97, i64 32) #5
  %add101 = add i64 %xor89, %or.i451
  %or.i454 = tail call i64 @llvm.fshl.i64(i64 %xor89, i64 %xor89, i64 16) #5
  %xor103 = xor i64 %or.i454, %add101
  %add104 = add i64 %xor103, %or.i453
  %or.i455 = tail call i64 @llvm.fshl.i64(i64 %xor103, i64 %xor103, i64 21) #5
  %xor106 = xor i64 %or.i455, %add104
  %add107 = add i64 %add101, %xor99
  %or.i456 = tail call i64 @llvm.fshl.i64(i64 %xor99, i64 %xor99, i64 17) #5
  %xor109 = xor i64 %or.i456, %add107
  %or.i457 = tail call i64 @llvm.fshl.i64(i64 %add107, i64 %add107, i64 32) #5
  %xor113 = xor i64 %add104, 1152921504606846976
  %xor114 = xor i64 %or.i457, 255
  %add116 = add i64 %xor113, %xor109
  %or.i458 = tail call i64 @llvm.fshl.i64(i64 %xor109, i64 %xor109, i64 13) #5
  %xor118 = xor i64 %add116, %or.i458
  %or.i459 = tail call i64 @llvm.fshl.i64(i64 %add116, i64 %add116, i64 32) #5
  %add120 = add i64 %xor114, %xor106
  %or.i460 = tail call i64 @llvm.fshl.i64(i64 %xor106, i64 %xor106, i64 16) #5
  %xor122 = xor i64 %or.i460, %add120
  %add123 = add i64 %or.i459, %xor122
  %or.i461 = tail call i64 @llvm.fshl.i64(i64 %xor122, i64 %xor122, i64 21) #5
  %xor125 = xor i64 %or.i461, %add123
  %add126 = add i64 %xor118, %add120
  %or.i462 = tail call i64 @llvm.fshl.i64(i64 %xor118, i64 %xor118, i64 17) #5
  %xor128 = xor i64 %or.i462, %add126
  %or.i463 = tail call i64 @llvm.fshl.i64(i64 %add126, i64 %add126, i64 32) #5
  %add133 = add i64 %xor128, %add123
  %or.i464 = tail call i64 @llvm.fshl.i64(i64 %xor128, i64 %xor128, i64 13) #5
  %xor135 = xor i64 %or.i464, %add133
  %or.i465 = tail call i64 @llvm.fshl.i64(i64 %add133, i64 %add133, i64 32) #5
  %add137 = add i64 %or.i463, %xor125
  %or.i466 = tail call i64 @llvm.fshl.i64(i64 %xor125, i64 %xor125, i64 16) #5
  %xor139 = xor i64 %or.i466, %add137
  %add140 = add i64 %or.i465, %xor139
  %or.i467 = tail call i64 @llvm.fshl.i64(i64 %xor139, i64 %xor139, i64 21) #5
  %xor142 = xor i64 %or.i467, %add140
  %add143 = add i64 %xor135, %add137
  %or.i468 = tail call i64 @llvm.fshl.i64(i64 %xor135, i64 %xor135, i64 17) #5
  %xor145 = xor i64 %or.i468, %add143
  %or.i469 = tail call i64 @llvm.fshl.i64(i64 %add143, i64 %add143, i64 32) #5
  %add150 = add i64 %xor145, %add140
  %or.i470 = tail call i64 @llvm.fshl.i64(i64 %xor145, i64 %xor145, i64 13) #5
  %xor152 = xor i64 %or.i470, %add150
  %or.i471 = tail call i64 @llvm.fshl.i64(i64 %add150, i64 %add150, i64 32) #5
  %add154 = add i64 %or.i469, %xor142
  %or.i472 = tail call i64 @llvm.fshl.i64(i64 %xor142, i64 %xor142, i64 16) #5
  %xor156 = xor i64 %or.i472, %add154
  %add157 = add i64 %or.i471, %xor156
  %or.i473 = tail call i64 @llvm.fshl.i64(i64 %xor156, i64 %xor156, i64 21) #5
  %xor159 = xor i64 %or.i473, %add157
  %add160 = add i64 %xor152, %add154
  %or.i474 = tail call i64 @llvm.fshl.i64(i64 %xor152, i64 %xor152, i64 17) #5
  %xor162 = xor i64 %or.i474, %add160
  %or.i475 = tail call i64 @llvm.fshl.i64(i64 %add160, i64 %add160, i64 32) #5
  %add167 = add i64 %xor162, %add157
  %or.i476 = tail call i64 @llvm.fshl.i64(i64 %xor162, i64 %xor162, i64 13) #5
  %xor169 = xor i64 %or.i476, %add167
  %add171 = add i64 %or.i475, %xor159
  %or.i478 = tail call i64 @llvm.fshl.i64(i64 %xor159, i64 %xor159, i64 16) #5
  %xor173 = xor i64 %or.i478, %add171
  %or.i479 = tail call i64 @llvm.fshl.i64(i64 %xor173, i64 %xor173, i64 21) #5
  %add177 = add i64 %xor169, %add171
  %or.i480 = tail call i64 @llvm.fshl.i64(i64 %xor169, i64 %xor169, i64 17) #5
  %xor179 = xor i64 %or.i480, %add177
  %or.i481 = tail call i64 @llvm.fshl.i64(i64 %add177, i64 %add177, i64 32) #5
  %xor183 = xor i64 %xor179, %or.i479
  %xor185 = xor i64 %xor183, %or.i481
  ret i64 %xor185
}

; Function Attrs: argmemonly mustprogress nofree nosync nounwind null_pointer_is_valid readonly sanitize_address sspstrong willreturn uwtable(sync)
define dso_local i64 @siphash_3u64(i64 noundef %first, i64 noundef %second, i64 noundef %third, ptr nocapture noundef readonly %key) #1 align 64 {
entry:
  call void @__sanitizer_cov_trace_pc() #4
  %arrayidx = getelementptr [2 x i64], ptr %key, i32 0, i32 1
  %0 = ptrtoint ptr %arrayidx to i32
  call void @__asan_load8_noabort(i32 %0)
  %1 = load i64, ptr %arrayidx, align 8
  %2 = ptrtoint ptr %key to i32
  call void @__asan_load8_noabort(i32 %2)
  %3 = load i64, ptr %key, align 8
  %xor4 = xor i64 %3, 7816392313619706465
  %xor7 = xor i64 %1, 7237128888997146477
  %xor10 = xor i64 %3, 8317987319222330741
  %4 = xor i64 %1, %first
  %xor11 = xor i64 %4, 8387220255154660723
  %add = add i64 %xor10, %xor7
  %or.i = tail call i64 @llvm.fshl.i64(i64 %xor7, i64 %xor7, i64 13) #5
  %xor12 = xor i64 %add, %or.i
  %or.i506 = tail call i64 @llvm.fshl.i64(i64 %add, i64 %add, i64 32) #5
  %add14 = add i64 %xor4, %xor11
  %or.i507 = tail call i64 @llvm.fshl.i64(i64 %xor11, i64 %xor11, i64 16) #5
  %xor16 = xor i64 %add14, %or.i507
  %add17 = add i64 %or.i506, %xor16
  %or.i508 = tail call i64 @llvm.fshl.i64(i64 %xor16, i64 %xor16, i64 21) #5
  %xor19 = xor i64 %or.i508, %add17
  %add20 = add i64 %xor12, %add14
  %or.i509 = tail call i64 @llvm.fshl.i64(i64 %xor12, i64 %xor12, i64 17) #5
  %xor22 = xor i64 %or.i509, %add20
  %or.i510 = tail call i64 @llvm.fshl.i64(i64 %add20, i64 %add20, i64 32) #5
  %add25 = add i64 %xor22, %add17
  %or.i511 = tail call i64 @llvm.fshl.i64(i64 %xor22, i64 %xor22, i64 13) #5
  %xor27 = xor i64 %or.i511, %add25
  %or.i512 = tail call i64 @llvm.fshl.i64(i64 %add25, i64 %add25, i64 32) #5
  %add29 = add i64 %or.i510, %xor19
  %or.i513 = tail call i64 @llvm.fshl.i64(i64 %xor19, i64 %xor19, i64 16) #5
  %xor31 = xor i64 %or.i513, %add29
  %add32 = add i64 %or.i512, %xor31
  %or.i514 = tail call i64 @llvm.fshl.i64(i64 %xor31, i64 %xor31, i64 21) #5
  %add35 = add i64 %xor27, %add29
  %or.i515 = tail call i64 @llvm.fshl.i64(i64 %xor27, i64 %xor27, i64 17) #5
  %xor37 = xor i64 %or.i515, %add35
  %or.i516 = tail call i64 @llvm.fshl.i64(i64 %add35, i64 %add35, i64 32) #5
  %xor41 = xor i64 %add32, %first
  %xor34 = xor i64 %add32, %second
  %xor42 = xor i64 %xor34, %or.i514
  %add44 = add i64 %xor41, %xor37
  %or.i517 = tail call i64 @llvm.fshl.i64(i64 %xor37, i64 %xor37, i64 13) #5
  %xor46 = xor i64 %or.i517, %add44
  %or.i518 = tail call i64 @llvm.fshl.i64(i64 %add44, i64 %add44, i64 32) #5
  %add48 = add i64 %xor42, %or.i516
  %or.i519 = tail call i64 @llvm.fshl.i64(i64 %xor42, i64 %xor42, i64 16) #5
  %xor50 = xor i64 %or.i519, %add48
  %add51 = add i64 %xor50, %or.i518
  %or.i520 = tail call i64 @llvm.fshl.i64(i64 %xor50, i64 %xor50, i64 21) #5
  %xor53 = xor i64 %or.i520, %add51
  %add54 = add i64 %add48, %xor46
  %or.i521 = tail call i64 @llvm.fshl.i64(i64 %xor46, i64 %xor46, i64 17) #5
  %xor56 = xor i64 %or.i521, %add54
  %or.i522 = tail call i64 @llvm.fshl.i64(i64 %add54, i64 %add54, i64 32) #5
  %add61 = add i64 %add51, %xor56
  %or.i523 = tail call i64 @llvm.fshl.i64(i64 %xor56, i64 %xor56, i64 13) #5
  %xor63 = xor i64 %or.i523, %add61
  %or.i524 = tail call i64 @llvm.fshl.i64(i64 %add61, i64 %add61, i64 32) #5
  %add65 = add i64 %xor53, %or.i522
  %or.i525 = tail call i64 @llvm.fshl.i64(i64 %xor53, i64 %xor53, i64 16) #5
  %xor67 = xor i64 %or.i525, %add65
  %add68 = add i64 %xor67, %or.i524
  %or.i526 = tail call i64 @llvm.fshl.i64(i64 %xor67, i64 %xor67, i64 21) #5
  %add71 = add i64 %add65, %xor63
  %or.i527 = tail call i64 @llvm.fshl.i64(i64 %xor63, i64 %xor63, i64 17) #5
  %xor73 = xor i64 %or.i527, %add71
  %or.i528 = tail call i64 @llvm.fshl.i64(i64 %add71, i64 %add71, i64 32) #5
  %xor77 = xor i64 %add68, %second
  %xor70 = xor i64 %add68, %third
  %xor78 = xor i64 %xor70, %or.i526
  %add80 = add i64 %xor77, %xor73
  %or.i529 = tail call i64 @llvm.fshl.i64(i64 %xor73, i64 %xor73, i64 13) #5
  %xor82 = xor i64 %add80, %or.i529
  %or.i530 = tail call i64 @llvm.fshl.i64(i64 %add80, i64 %add80, i64 32) #5
  %add84 = add i64 %xor78, %or.i528
  %or.i531 = tail call i64 @llvm.fshl.i64(i64 %xor78, i64 %xor78, i64 16) #5
  %xor86 = xor i64 %or.i531, %add84
  %add87 = add i64 %xor86, %or.i530
  %or.i532 = tail call i64 @llvm.fshl.i64(i64 %xor86, i64 %xor86, i64 21) #5
  %xor89 = xor i64 %or.i532, %add87
  %add90 = add i64 %add84, %xor82
  %or.i533 = tail call i64 @llvm.fshl.i64(i64 %xor82, i64 %xor82, i64 17) #5
  %xor92 = xor i64 %or.i533, %add90
  %or.i534 = tail call i64 @llvm.fshl.i64(i64 %add90, i64 %add90, i64 32) #5
  %add97 = add i64 %add87, %xor92
  %or.i535 = tail call i64 @llvm.fshl.i64(i64 %xor92, i64 %xor92, i64 13) #5
  %xor99 = xor i64 %or.i535, %add97
  %or.i536 = tail call i64 @llvm.fshl.i64(i64 %add97, i64 %add97, i64 32) #5
  %add101 = add i64 %xor89, %or.i534
  %or.i537 = tail call i64 @llvm.fshl.i64(i64 %xor89, i64 %xor89, i64 16) #5
  %xor103 = xor i64 %or.i537, %add101
  %add104 = add i64 %xor103, %or.i536
  %or.i538 = tail call i64 @llvm.fshl.i64(i64 %xor103, i64 %xor103, i64 21) #5
  %xor106 = xor i64 %or.i538, %add104
  %add107 = add i64 %add101, %xor99
  %or.i539 = tail call i64 @llvm.fshl.i64(i64 %xor99, i64 %xor99, i64 17) #5
  %xor109 = xor i64 %or.i539, %add107
  %or.i540 = tail call i64 @llvm.fshl.i64(i64 %add107, i64 %add107, i64 32) #5
  %xor113 = xor i64 %add104, %third
  %xor114 = xor i64 %xor106, 1729382256910270464
  %add116 = add i64 %xor113, %xor109
  %or.i541 = tail call i64 @llvm.fshl.i64(i64 %xor109, i64 %xor109, i64 13) #5
  %xor118 = xor i64 %add116, %or.i541
  %or.i542 = tail call i64 @llvm.fshl.i64(i64 %add116, i64 %add116, i64 32) #5
  %add120 = add i64 %xor114, %or.i540
  %or.i543 = tail call i64 @llvm.fshl.i64(i64 %xor106, i64 %xor114, i64 16) #5
  %xor122 = xor i64 %or.i543, %add120
  %add123 = add i64 %xor122, %or.i542
  %or.i544 = tail call i64 @llvm.fshl.i64(i64 %xor122, i64 %xor122, i64 21) #5
  %xor125 = xor i64 %or.i544, %add123
  %add126 = add i64 %add120, %xor118
  %or.i545 = tail call i64 @llvm.fshl.i64(i64 %xor118, i64 %xor118, i64 17) #5
  %xor128 = xor i64 %or.i545, %add126
  %or.i546 = tail call i64 @llvm.fshl.i64(i64 %add126, i64 %add126, i64 32) #5
  %add133 = add i64 %add123, %xor128
  %or.i547 = tail call i64 @llvm.fshl.i64(i64 %xor128, i64 %xor128, i64 13) #5
  %xor135 = xor i64 %or.i547, %add133
  %or.i548 = tail call i64 @llvm.fshl.i64(i64 %add133, i64 %add133, i64 32) #5
  %add137 = add i64 %xor125, %or.i546
  %or.i549 = tail call i64 @llvm.fshl.i64(i64 %xor125, i64 %xor125, i64 16) #5
  %xor139 = xor i64 %or.i549, %add137
  %add140 = add i64 %xor139, %or.i548
  %or.i550 = tail call i64 @llvm.fshl.i64(i64 %xor139, i64 %xor139, i64 21) #5
  %xor142 = xor i64 %or.i550, %add140
  %add143 = add i64 %add137, %xor135
  %or.i551 = tail call i64 @llvm.fshl.i64(i64 %xor135, i64 %xor135, i64 17) #5
  %xor145 = xor i64 %or.i551, %add143
  %or.i552 = tail call i64 @llvm.fshl.i64(i64 %add143, i64 %add143, i64 32) #5
  %xor149 = xor i64 %add140, 1729382256910270464
  %xor150 = xor i64 %or.i552, 255
  %add152 = add i64 %xor149, %xor145
  %or.i553 = tail call i64 @llvm.fshl.i64(i64 %xor145, i64 %xor145, i64 13) #5
  %xor154 = xor i64 %add152, %or.i553
  %or.i554 = tail call i64 @llvm.fshl.i64(i64 %add152, i64 %add152, i64 32) #5
  %add156 = add i64 %xor150, %xor142
  %or.i555 = tail call i64 @llvm.fshl.i64(i64 %xor142, i64 %xor142, i64 16) #5
  %xor158 = xor i64 %or.i555, %add156
  %add159 = add i64 %or.i554, %xor158
  %or.i556 = tail call i64 @llvm.fshl.i64(i64 %xor158, i64 %xor158, i64 21) #5
  %xor161 = xor i64 %or.i556, %add159
  %add162 = add i64 %xor154, %add156
  %or.i557 = tail call i64 @llvm.fshl.i64(i64 %xor154, i64 %xor154, i64 17) #5
  %xor164 = xor i64 %or.i557, %add162
  %or.i558 = tail call i64 @llvm.fshl.i64(i64 %add162, i64 %add162, i64 32) #5
  %add169 = add i64 %xor164, %add159
  %or.i559 = tail call i64 @llvm.fshl.i64(i64 %xor164, i64 %xor164, i64 13) #5
  %xor171 = xor i64 %or.i559, %add169
  %or.i560 = tail call i64 @llvm.fshl.i64(i64 %add169, i64 %add169, i64 32) #5
  %add173 = add i64 %or.i558, %xor161
  %or.i561 = tail call i64 @llvm.fshl.i64(i64 %xor161, i64 %xor161, i64 16) #5
  %xor175 = xor i64 %or.i561, %add173
  %add176 = add i64 %or.i560, %xor175
  %or.i562 = tail call i64 @llvm.fshl.i64(i64 %xor175, i64 %xor175, i64 21) #5
  %xor178 = xor i64 %or.i562, %add176
  %add179 = add i64 %xor171, %add173
  %or.i563 = tail call i64 @llvm.fshl.i64(i64 %xor171, i64 %xor171, i64 17) #5
  %xor181 = xor i64 %or.i563, %add179
  %or.i564 = tail call i64 @llvm.fshl.i64(i64 %add179, i64 %add179, i64 32) #5
  %add186 = add i64 %xor181, %add176
  %or.i565 = tail call i64 @llvm.fshl.i64(i64 %xor181, i64 %xor181, i64 13) #5
  %xor188 = xor i64 %or.i565, %add186
  %or.i566 = tail call i64 @llvm.fshl.i64(i64 %add186, i64 %add186, i64 32) #5
  %add190 = add i64 %or.i564, %xor178
  %or.i567 = tail call i64 @llvm.fshl.i64(i64 %xor178, i64 %xor178, i64 16) #5
  %xor192 = xor i64 %or.i567, %add190
  %add193 = add i64 %or.i566, %xor192
  %or.i568 = tail call i64 @llvm.fshl.i64(i64 %xor192, i64 %xor192, i64 21) #5
  %xor195 = xor i64 %or.i568, %add193
  %add196 = add i64 %xor188, %add190
  %or.i569 = tail call i64 @llvm.fshl.i64(i64 %xor188, i64 %xor188, i64 17) #5
  %xor198 = xor i64 %or.i569, %add196
  %or.i570 = tail call i64 @llvm.fshl.i64(i64 %add196, i64 %add196, i64 32) #5
  %add203 = add i64 %xor198, %add193
  %or.i571 = tail call i64 @llvm.fshl.i64(i64 %xor198, i64 %xor198, i64 13) #5
  %xor205 = xor i64 %or.i571, %add203
  %add207 = add i64 %or.i570, %xor195
  %or.i573 = tail call i64 @llvm.fshl.i64(i64 %xor195, i64 %xor195, i64 16) #5
  %xor209 = xor i64 %or.i573, %add207
  %or.i574 = tail call i64 @llvm.fshl.i64(i64 %xor209, i64 %xor209, i64 21) #5
  %add213 = add i64 %xor205, %add207
  %or.i575 = tail call i64 @llvm.fshl.i64(i64 %xor205, i64 %xor205, i64 17) #5
  %xor215 = xor i64 %or.i575, %add213
  %or.i576 = tail call i64 @llvm.fshl.i64(i64 %add213, i64 %add213, i64 32) #5
  %xor219 = xor i64 %xor215, %or.i574
  %xor221 = xor i64 %xor219, %or.i576
  ret i64 %xor221
}

; Function Attrs: argmemonly mustprogress nofree nosync nounwind null_pointer_is_valid readonly sanitize_address sspstrong willreturn uwtable(sync)
define dso_local i64 @siphash_4u64(i64 noundef %first, i64 noundef %second, i64 noundef %third, i64 noundef %forth, ptr nocapture noundef readonly %key) #1 align 64 {
entry:
  call void @__sanitizer_cov_trace_pc() #4
  %arrayidx = getelementptr [2 x i64], ptr %key, i32 0, i32 1
  %0 = ptrtoint ptr %arrayidx to i32
  call void @__asan_load8_noabort(i32 %0)
  %1 = load i64, ptr %arrayidx, align 8
  %2 = ptrtoint ptr %key to i32
  call void @__asan_load8_noabort(i32 %2)
  %3 = load i64, ptr %key, align 8
  %xor4 = xor i64 %3, 7816392313619706465
  %xor7 = xor i64 %1, 7237128888997146477
  %xor10 = xor i64 %3, 8317987319222330741
  %4 = xor i64 %1, %first
  %xor11 = xor i64 %4, 8387220255154660723
  %add = add i64 %xor10, %xor7
  %or.i = tail call i64 @llvm.fshl.i64(i64 %xor7, i64 %xor7, i64 13) #5
  %xor12 = xor i64 %add, %or.i
  %or.i589 = tail call i64 @llvm.fshl.i64(i64 %add, i64 %add, i64 32) #5
  %add14 = add i64 %xor4, %xor11
  %or.i590 = tail call i64 @llvm.fshl.i64(i64 %xor11, i64 %xor11, i64 16) #5
  %xor16 = xor i64 %add14, %or.i590
  %add17 = add i64 %or.i589, %xor16
  %or.i591 = tail call i64 @llvm.fshl.i64(i64 %xor16, i64 %xor16, i64 21) #5
  %xor19 = xor i64 %or.i591, %add17
  %add20 = add i64 %xor12, %add14
  %or.i592 = tail call i64 @llvm.fshl.i64(i64 %xor12, i64 %xor12, i64 17) #5
  %xor22 = xor i64 %or.i592, %add20
  %or.i593 = tail call i64 @llvm.fshl.i64(i64 %add20, i64 %add20, i64 32) #5
  %add25 = add i64 %xor22, %add17
  %or.i594 = tail call i64 @llvm.fshl.i64(i64 %xor22, i64 %xor22, i64 13) #5
  %xor27 = xor i64 %or.i594, %add25
  %or.i595 = tail call i64 @llvm.fshl.i64(i64 %add25, i64 %add25, i64 32) #5
  %add29 = add i64 %or.i593, %xor19
  %or.i596 = tail call i64 @llvm.fshl.i64(i64 %xor19, i64 %xor19, i64 16) #5
  %xor31 = xor i64 %or.i596, %add29
  %add32 = add i64 %or.i595, %xor31
  %or.i597 = tail call i64 @llvm.fshl.i64(i64 %xor31, i64 %xor31, i64 21) #5
  %add35 = add i64 %xor27, %add29
  %or.i598 = tail call i64 @llvm.fshl.i64(i64 %xor27, i64 %xor27, i64 17) #5
  %xor37 = xor i64 %or.i598, %add35
  %or.i599 = tail call i64 @llvm.fshl.i64(i64 %add35, i64 %add35, i64 32) #5
  %xor41 = xor i64 %add32, %first
  %xor34 = xor i64 %add32, %second
  %xor42 = xor i64 %xor34, %or.i597
  %add44 = add i64 %xor41, %xor37
  %or.i600 = tail call i64 @llvm.fshl.i64(i64 %xor37, i64 %xor37, i64 13) #5
  %xor46 = xor i64 %or.i600, %add44
  %or.i601 = tail call i64 @llvm.fshl.i64(i64 %add44, i64 %add44, i64 32) #5
  %add48 = add i64 %xor42, %or.i599
  %or.i602 = tail call i64 @llvm.fshl.i64(i64 %xor42, i64 %xor42, i64 16) #5
  %xor50 = xor i64 %or.i602, %add48
  %add51 = add i64 %xor50, %or.i601
  %or.i603 = tail call i64 @llvm.fshl.i64(i64 %xor50, i64 %xor50, i64 21) #5
  %xor53 = xor i64 %or.i603, %add51
  %add54 = add i64 %add48, %xor46
  %or.i604 = tail call i64 @llvm.fshl.i64(i64 %xor46, i64 %xor46, i64 17) #5
  %xor56 = xor i64 %or.i604, %add54
  %or.i605 = tail call i64 @llvm.fshl.i64(i64 %add54, i64 %add54, i64 32) #5
  %add61 = add i64 %add51, %xor56
  %or.i606 = tail call i64 @llvm.fshl.i64(i64 %xor56, i64 %xor56, i64 13) #5
  %xor63 = xor i64 %or.i606, %add61
  %or.i607 = tail call i64 @llvm.fshl.i64(i64 %add61, i64 %add61, i64 32) #5
  %add65 = add i64 %xor53, %or.i605
  %or.i608 = tail call i64 @llvm.fshl.i64(i64 %xor53, i64 %xor53, i64 16) #5
  %xor67 = xor i64 %or.i608, %add65
  %add68 = add i64 %xor67, %or.i607
  %or.i609 = tail call i64 @llvm.fshl.i64(i64 %xor67, i64 %xor67, i64 21) #5
  %add71 = add i64 %add65, %xor63
  %or.i610 = tail call i64 @llvm.fshl.i64(i64 %xor63, i64 %xor63, i64 17) #5
  %xor73 = xor i64 %or.i610, %add71
  %or.i611 = tail call i64 @llvm.fshl.i64(i64 %add71, i64 %add71, i64 32) #5
  %xor77 = xor i64 %add68, %second
  %xor70 = xor i64 %add68, %third
  %xor78 = xor i64 %xor70, %or.i609
  %add80 = add i64 %xor77, %xor73
  %or.i612 = tail call i64 @llvm.fshl.i64(i64 %xor73, i64 %xor73, i64 13) #5
  %xor82 = xor i64 %add80, %or.i612
  %or.i613 = tail call i64 @llvm.fshl.i64(i64 %add80, i64 %add80, i64 32) #5
  %add84 = add i64 %xor78, %or.i611
  %or.i614 = tail call i64 @llvm.fshl.i64(i64 %xor78, i64 %xor78, i64 16) #5
  %xor86 = xor i64 %or.i614, %add84
  %add87 = add i64 %xor86, %or.i613
  %or.i615 = tail call i64 @llvm.fshl.i64(i64 %xor86, i64 %xor86, i64 21) #5
  %xor89 = xor i64 %or.i615, %add87
  %add90 = add i64 %add84, %xor82
  %or.i616 = tail call i64 @llvm.fshl.i64(i64 %xor82, i64 %xor82, i64 17) #5
  %xor92 = xor i64 %or.i616, %add90
  %or.i617 = tail call i64 @llvm.fshl.i64(i64 %add90, i64 %add90, i64 32) #5
  %add97 = add i64 %add87, %xor92
  %or.i618 = tail call i64 @llvm.fshl.i64(i64 %xor92, i64 %xor92, i64 13) #5
  %xor99 = xor i64 %or.i618, %add97
  %or.i619 = tail call i64 @llvm.fshl.i64(i64 %add97, i64 %add97, i64 32) #5
  %add101 = add i64 %xor89, %or.i617
  %or.i620 = tail call i64 @llvm.fshl.i64(i64 %xor89, i64 %xor89, i64 16) #5
  %xor103 = xor i64 %or.i620, %add101
  %add104 = add i64 %xor103, %or.i619
  %or.i621 = tail call i64 @llvm.fshl.i64(i64 %xor103, i64 %xor103, i64 21) #5
  %add107 = add i64 %add101, %xor99
  %or.i622 = tail call i64 @llvm.fshl.i64(i64 %xor99, i64 %xor99, i64 17) #5
  %xor109 = xor i64 %or.i622, %add107
  %or.i623 = tail call i64 @llvm.fshl.i64(i64 %add107, i64 %add107, i64 32) #5
  %xor113 = xor i64 %add104, %third
  %xor106 = xor i64 %add104, %forth
  %xor114 = xor i64 %xor106, %or.i621
  %add116 = add i64 %xor113, %xor109
  %or.i624 = tail call i64 @llvm.fshl.i64(i64 %xor109, i64 %xor109, i64 13) #5
  %xor118 = xor i64 %add116, %or.i624
  %or.i625 = tail call i64 @llvm.fshl.i64(i64 %add116, i64 %add116, i64 32) #5
  %add120 = add i64 %xor114, %or.i623
  %or.i626 = tail call i64 @llvm.fshl.i64(i64 %xor114, i64 %xor114, i64 16) #5
  %xor122 = xor i64 %or.i626, %add120
  %add123 = add i64 %xor122, %or.i625
  %or.i627 = tail call i64 @llvm.fshl.i64(i64 %xor122, i64 %xor122, i64 21) #5
  %xor125 = xor i64 %or.i627, %add123
  %add126 = add i64 %add120, %xor118
  %or.i628 = tail call i64 @llvm.fshl.i64(i64 %xor118, i64 %xor118, i64 17) #5
  %xor128 = xor i64 %or.i628, %add126
  %or.i629 = tail call i64 @llvm.fshl.i64(i64 %add126, i64 %add126, i64 32) #5
  %add133 = add i64 %add123, %xor128
  %or.i630 = tail call i64 @llvm.fshl.i64(i64 %xor128, i64 %xor128, i64 13) #5
  %xor135 = xor i64 %or.i630, %add133
  %or.i631 = tail call i64 @llvm.fshl.i64(i64 %add133, i64 %add133, i64 32) #5
  %add137 = add i64 %xor125, %or.i629
  %or.i632 = tail call i64 @llvm.fshl.i64(i64 %xor125, i64 %xor125, i64 16) #5
  %xor139 = xor i64 %or.i632, %add137
  %add140 = add i64 %xor139, %or.i631
  %or.i633 = tail call i64 @llvm.fshl.i64(i64 %xor139, i64 %xor139, i64 21) #5
  %xor142 = xor i64 %or.i633, %add140
  %add143 = add i64 %add137, %xor135
  %or.i634 = tail call i64 @llvm.fshl.i64(i64 %xor135, i64 %xor135, i64 17) #5
  %xor145 = xor i64 %or.i634, %add143
  %or.i635 = tail call i64 @llvm.fshl.i64(i64 %add143, i64 %add143, i64 32) #5
  %xor149 = xor i64 %add140, %forth
  %xor150 = xor i64 %xor142, 2305843009213693952
  %add152 = add i64 %xor149, %xor145
  %or.i636 = tail call i64 @llvm.fshl.i64(i64 %xor145, i64 %xor145, i64 13) #5
  %xor154 = xor i64 %add152, %or.i636
  %or.i637 = tail call i64 @llvm.fshl.i64(i64 %add152, i64 %add152, i64 32) #5
  %add156 = add i64 %xor150, %or.i635
  %or.i638 = tail call i64 @llvm.fshl.i64(i64 %xor142, i64 %xor150, i64 16) #5
  %xor158 = xor i64 %or.i638, %add156
  %add159 = add i64 %xor158, %or.i637
  %or.i639 = tail call i64 @llvm.fshl.i64(i64 %xor158, i64 %xor158, i64 21) #5
  %xor161 = xor i64 %or.i639, %add159
  %add162 = add i64 %add156, %xor154
  %or.i640 = tail call i64 @llvm.fshl.i64(i64 %xor154, i64 %xor154, i64 17) #5
  %xor164 = xor i64 %or.i640, %add162
  %or.i641 = tail call i64 @llvm.fshl.i64(i64 %add162, i64 %add162, i64 32) #5
  %add169 = add i64 %add159, %xor164
  %or.i642 = tail call i64 @llvm.fshl.i64(i64 %xor164, i64 %xor164, i64 13) #5
  %xor171 = xor i64 %or.i642, %add169
  %or.i643 = tail call i64 @llvm.fshl.i64(i64 %add169, i64 %add169, i64 32) #5
  %add173 = add i64 %xor161, %or.i641
  %or.i644 = tail call i64 @llvm.fshl.i64(i64 %xor161, i64 %xor161, i64 16) #5
  %xor175 = xor i64 %or.i644, %add173
  %add176 = add i64 %xor175, %or.i643
  %or.i645 = tail call i64 @llvm.fshl.i64(i64 %xor175, i64 %xor175, i64 21) #5
  %xor178 = xor i64 %or.i645, %add176
  %add179 = add i64 %add173, %xor171
  %or.i646 = tail call i64 @llvm.fshl.i64(i64 %xor171, i64 %xor171, i64 17) #5
  %xor181 = xor i64 %or.i646, %add179
  %or.i647 = tail call i64 @llvm.fshl.i64(i64 %add179, i64 %add179, i64 32) #5
  %xor185 = xor i64 %add176, 2305843009213693952
  %xor186 = xor i64 %or.i647, 255
  %add188 = add i64 %xor185, %xor181
  %or.i648 = tail call i64 @llvm.fshl.i64(i64 %xor181, i64 %xor181, i64 13) #5
  %xor190 = xor i64 %add188, %or.i648
  %or.i649 = tail call i64 @llvm.fshl.i64(i64 %add188, i64 %add188, i64 32) #5
  %add192 = add i64 %xor186, %xor178
  %or.i650 = tail call i64 @llvm.fshl.i64(i64 %xor178, i64 %xor178, i64 16) #5
  %xor194 = xor i64 %or.i650, %add192
  %add195 = add i64 %or.i649, %xor194
  %or.i651 = tail call i64 @llvm.fshl.i64(i64 %xor194, i64 %xor194, i64 21) #5
  %xor197 = xor i64 %or.i651, %add195
  %add198 = add i64 %xor190, %add192
  %or.i652 = tail call i64 @llvm.fshl.i64(i64 %xor190, i64 %xor190, i64 17) #5
  %xor200 = xor i64 %or.i652, %add198
  %or.i653 = tail call i64 @llvm.fshl.i64(i64 %add198, i64 %add198, i64 32) #5
  %add205 = add i64 %xor200, %add195
  %or.i654 = tail call i64 @llvm.fshl.i64(i64 %xor200, i64 %xor200, i64 13) #5
  %xor207 = xor i64 %or.i654, %add205
  %or.i655 = tail call i64 @llvm.fshl.i64(i64 %add205, i64 %add205, i64 32) #5
  %add209 = add i64 %or.i653, %xor197
  %or.i656 = tail call i64 @llvm.fshl.i64(i64 %xor197, i64 %xor197, i64 16) #5
  %xor211 = xor i64 %or.i656, %add209
  %add212 = add i64 %or.i655, %xor211
  %or.i657 = tail call i64 @llvm.fshl.i64(i64 %xor211, i64 %xor211, i64 21) #5
  %xor214 = xor i64 %or.i657, %add212
  %add215 = add i64 %xor207, %add209
  %or.i658 = tail call i64 @llvm.fshl.i64(i64 %xor207, i64 %xor207, i64 17) #5
  %xor217 = xor i64 %or.i658, %add215
  %or.i659 = tail call i64 @llvm.fshl.i64(i64 %add215, i64 %add215, i64 32) #5
  %add222 = add i64 %xor217, %add212
  %or.i660 = tail call i64 @llvm.fshl.i64(i64 %xor217, i64 %xor217, i64 13) #5
  %xor224 = xor i64 %or.i660, %add222
  %or.i661 = tail call i64 @llvm.fshl.i64(i64 %add222, i64 %add222, i64 32) #5
  %add226 = add i64 %or.i659, %xor214
  %or.i662 = tail call i64 @llvm.fshl.i64(i64 %xor214, i64 %xor214, i64 16) #5
  %xor228 = xor i64 %or.i662, %add226
  %add229 = add i64 %or.i661, %xor228
  %or.i663 = tail call i64 @llvm.fshl.i64(i64 %xor228, i64 %xor228, i64 21) #5
  %xor231 = xor i64 %or.i663, %add229
  %add232 = add i64 %xor224, %add226
  %or.i664 = tail call i64 @llvm.fshl.i64(i64 %xor224, i64 %xor224, i64 17) #5
  %xor234 = xor i64 %or.i664, %add232
  %or.i665 = tail call i64 @llvm.fshl.i64(i64 %add232, i64 %add232, i64 32) #5
  %add239 = add i64 %xor234, %add229
  %or.i666 = tail call i64 @llvm.fshl.i64(i64 %xor234, i64 %xor234, i64 13) #5
  %xor241 = xor i64 %or.i666, %add239
  %add243 = add i64 %or.i665, %xor231
  %or.i668 = tail call i64 @llvm.fshl.i64(i64 %xor231, i64 %xor231, i64 16) #5
  %xor245 = xor i64 %or.i668, %add243
  %or.i669 = tail call i64 @llvm.fshl.i64(i64 %xor245, i64 %xor245, i64 21) #5
  %add249 = add i64 %xor241, %add243
  %or.i670 = tail call i64 @llvm.fshl.i64(i64 %xor241, i64 %xor241, i64 17) #5
  %xor251 = xor i64 %or.i670, %add249
  %or.i671 = tail call i64 @llvm.fshl.i64(i64 %add249, i64 %add249, i64 32) #5
  %xor255 = xor i64 %xor251, %or.i669
  %xor257 = xor i64 %xor255, %or.i671
  ret i64 %xor257
}

; Function Attrs: argmemonly mustprogress nofree nosync nounwind null_pointer_is_valid readonly sanitize_address sspstrong willreturn uwtable(sync)
define dso_local i64 @siphash_1u32(i32 noundef %first, ptr nocapture noundef readonly %key) #1 align 64 {
entry:
  call void @__sanitizer_cov_trace_pc() #4
  %arrayidx = getelementptr [2 x i64], ptr %key, i32 0, i32 1
  %0 = ptrtoint ptr %arrayidx to i32
  call void @__asan_load8_noabort(i32 %0)
  %1 = load i64, ptr %arrayidx, align 8
  %2 = ptrtoint ptr %key to i32
  call void @__asan_load8_noabort(i32 %2)
  %3 = load i64, ptr %key, align 8
  %xor4 = xor i64 %3, 7816392313619706465
  %xor7 = xor i64 %1, 7237128888997146477
  %xor10 = xor i64 %3, 8317987319222330741
  %conv = zext i32 %first to i64
  %or = or i64 %conv, 288230376151711744
  %4 = xor i64 %1, %conv
  %xor11 = xor i64 %4, 8098989879002948979
  %add = add i64 %xor10, %xor7
  %or.i = tail call i64 @llvm.fshl.i64(i64 %xor7, i64 %xor7, i64 13) #5
  %xor12 = xor i64 %add, %or.i
  %or.i258 = tail call i64 @llvm.fshl.i64(i64 %add, i64 %add, i64 32) #5
  %add14 = add i64 %xor4, %xor11
  %or.i259 = tail call i64 @llvm.fshl.i64(i64 %xor11, i64 %xor11, i64 16) #5
  %xor16 = xor i64 %add14, %or.i259
  %add17 = add i64 %or.i258, %xor16
  %or.i260 = tail call i64 @llvm.fshl.i64(i64 %xor16, i64 %xor16, i64 21) #5
  %xor19 = xor i64 %or.i260, %add17
  %add20 = add i64 %xor12, %add14
  %or.i261 = tail call i64 @llvm.fshl.i64(i64 %xor12, i64 %xor12, i64 17) #5
  %xor22 = xor i64 %or.i261, %add20
  %or.i262 = tail call i64 @llvm.fshl.i64(i64 %add20, i64 %add20, i64 32) #5
  %add25 = add i64 %xor22, %add17
  %or.i263 = tail call i64 @llvm.fshl.i64(i64 %xor22, i64 %xor22, i64 13) #5
  %xor27 = xor i64 %or.i263, %add25
  %or.i264 = tail call i64 @llvm.fshl.i64(i64 %add25, i64 %add25, i64 32) #5
  %add29 = add i64 %or.i262, %xor19
  %or.i265 = tail call i64 @llvm.fshl.i64(i64 %xor19, i64 %xor19, i64 16) #5
  %xor31 = xor i64 %or.i265, %add29
  %add32 = add i64 %or.i264, %xor31
  %or.i266 = tail call i64 @llvm.fshl.i64(i64 %xor31, i64 %xor31, i64 21) #5
  %xor34 = xor i64 %or.i266, %add32
  %add35 = add i64 %xor27, %add29
  %or.i267 = tail call i64 @llvm.fshl.i64(i64 %xor27, i64 %xor27, i64 17) #5
  %xor37 = xor i64 %or.i267, %add35
  %or.i268 = tail call i64 @llvm.fshl.i64(i64 %add35, i64 %add35, i64 32) #5
  %xor41 = xor i64 %add32, %or
  %xor42 = xor i64 %or.i268, 255
  %add44 = add i64 %xor41, %xor37
  %or.i269 = tail call i64 @llvm.fshl.i64(i64 %xor37, i64 %xor37, i64 13) #5
  %xor46 = xor i64 %or.i269, %add44
  %or.i270 = tail call i64 @llvm.fshl.i64(i64 %add44, i64 %add44, i64 32) #5
  %add48 = add i64 %xor42, %xor34
  %or.i271 = tail call i64 @llvm.fshl.i64(i64 %xor34, i64 %xor34, i64 16) #5
  %xor50 = xor i64 %add48, %or.i271
  %add51 = add i64 %xor50, %or.i270
  %or.i272 = tail call i64 @llvm.fshl.i64(i64 %xor50, i64 %xor50, i64 21) #5
  %xor53 = xor i64 %or.i272, %add51
  %add54 = add i64 %add48, %xor46
  %or.i273 = tail call i64 @llvm.fshl.i64(i64 %xor46, i64 %xor46, i64 17) #5
  %xor56 = xor i64 %or.i273, %add54
  %or.i274 = tail call i64 @llvm.fshl.i64(i64 %add54, i64 %add54, i64 32) #5
  %add61 = add i64 %add51, %xor56
  %or.i275 = tail call i64 @llvm.fshl.i64(i64 %xor56, i64 %xor56, i64 13) #5
  %xor63 = xor i64 %or.i275, %add61
  %or.i276 = tail call i64 @llvm.fshl.i64(i64 %add61, i64 %add61, i64 32) #5
  %add65 = add i64 %xor53, %or.i274
  %or.i277 = tail call i64 @llvm.fshl.i64(i64 %xor53, i64 %xor53, i64 16) #5
  %xor67 = xor i64 %or.i277, %add65
  %add68 = add i64 %xor67, %or.i276
  %or.i278 = tail call i64 @llvm.fshl.i64(i64 %xor67, i64 %xor67, i64 21) #5
  %xor70 = xor i64 %or.i278, %add68
  %add71 = add i64 %add65, %xor63
  %or.i279 = tail call i64 @llvm.fshl.i64(i64 %xor63, i64 %xor63, i64 17) #5
  %xor73 = xor i64 %or.i279, %add71
  %or.i280 = tail call i64 @llvm.fshl.i64(i64 %add71, i64 %add71, i64 32) #5
  %add78 = add i64 %add68, %xor73
  %or.i281 = tail call i64 @llvm.fshl.i64(i64 %xor73, i64 %xor73, i64 13) #5
  %xor80 = xor i64 %or.i281, %add78
  %or.i282 = tail call i64 @llvm.fshl.i64(i64 %add78, i64 %add78, i64 32) #5
  %add82 = add i64 %xor70, %or.i280
  %or.i283 = tail call i64 @llvm.fshl.i64(i64 %xor70, i64 %xor70, i64 16) #5
  %xor84 = xor i64 %or.i283, %add82
  %add85 = add i64 %xor84, %or.i282
  %or.i284 = tail call i64 @llvm.fshl.i64(i64 %xor84, i64 %xor84, i64 21) #5
  %xor87 = xor i64 %or.i284, %add85
  %add88 = add i64 %add82, %xor80
  %or.i285 = tail call i64 @llvm.fshl.i64(i64 %xor80, i64 %xor80, i64 17) #5
  %xor90 = xor i64 %or.i285, %add88
  %or.i286 = tail call i64 @llvm.fshl.i64(i64 %add88, i64 %add88, i64 32) #5
  %add95 = add i64 %add85, %xor90
  %or.i287 = tail call i64 @llvm.fshl.i64(i64 %xor90, i64 %xor90, i64 13) #5
  %xor97 = xor i64 %or.i287, %add95
  %add99 = add i64 %xor87, %or.i286
  %or.i289 = tail call i64 @llvm.fshl.i64(i64 %xor87, i64 %xor87, i64 16) #5
  %xor101 = xor i64 %or.i289, %add99
  %or.i290 = tail call i64 @llvm.fshl.i64(i64 %xor101, i64 %xor101, i64 21) #5
  %add105 = add i64 %add99, %xor97
  %or.i291 = tail call i64 @llvm.fshl.i64(i64 %xor97, i64 %xor97, i64 17) #5
  %xor107 = xor i64 %or.i291, %add105
  %or.i292 = tail call i64 @llvm.fshl.i64(i64 %add105, i64 %add105, i64 32) #5
  %xor111 = xor i64 %xor107, %or.i290
  %xor113 = xor i64 %xor111, %or.i292
  ret i64 %xor113
}

; Function Attrs: argmemonly mustprogress nofree nosync nounwind null_pointer_is_valid readonly sanitize_address sspstrong willreturn uwtable(sync)
define dso_local i64 @siphash_3u32(i32 noundef %first, i32 noundef %second, i32 noundef %third, ptr nocapture noundef readonly %key) #1 align 64 {
entry:
  call void @__sanitizer_cov_trace_pc() #4
  %conv = zext i32 %second to i64
  %shl = shl nuw i64 %conv, 32
  %conv1 = zext i32 %first to i64
  %or = or i64 %shl, %conv1
  %arrayidx = getelementptr [2 x i64], ptr %key, i32 0, i32 1
  %0 = ptrtoint ptr %arrayidx to i32
  call void @__asan_load8_noabort(i32 %0)
  %1 = load i64, ptr %arrayidx, align 8
  %2 = ptrtoint ptr %key to i32
  call void @__asan_load8_noabort(i32 %2)
  %3 = load i64, ptr %key, align 8
  %xor5 = xor i64 %3, 7816392313619706465
  %xor8 = xor i64 %1, 7237128888997146477
  %xor11 = xor i64 %3, 8317987319222330741
  %4 = xor i64 %or, %1
  %xor12 = xor i64 %4, 8387220255154660723
  %add = add i64 %xor11, %xor8
  %or.i = tail call i64 @llvm.fshl.i64(i64 %xor8, i64 %xor8, i64 13) #5
  %xor13 = xor i64 %add, %or.i
  %or.i344 = tail call i64 @llvm.fshl.i64(i64 %add, i64 %add, i64 32) #5
  %add15 = add i64 %xor5, %xor12
  %or.i345 = tail call i64 @llvm.fshl.i64(i64 %xor12, i64 %xor12, i64 16) #5
  %xor17 = xor i64 %add15, %or.i345
  %add18 = add i64 %or.i344, %xor17
  %or.i346 = tail call i64 @llvm.fshl.i64(i64 %xor17, i64 %xor17, i64 21) #5
  %xor20 = xor i64 %or.i346, %add18
  %add21 = add i64 %xor13, %add15
  %or.i347 = tail call i64 @llvm.fshl.i64(i64 %xor13, i64 %xor13, i64 17) #5
  %xor23 = xor i64 %or.i347, %add21
  %or.i348 = tail call i64 @llvm.fshl.i64(i64 %add21, i64 %add21, i64 32) #5
  %add26 = add i64 %xor23, %add18
  %or.i349 = tail call i64 @llvm.fshl.i64(i64 %xor23, i64 %xor23, i64 13) #5
  %xor28 = xor i64 %or.i349, %add26
  %or.i350 = tail call i64 @llvm.fshl.i64(i64 %add26, i64 %add26, i64 32) #5
  %add30 = add i64 %or.i348, %xor20
  %or.i351 = tail call i64 @llvm.fshl.i64(i64 %xor20, i64 %xor20, i64 16) #5
  %xor32 = xor i64 %or.i351, %add30
  %add33 = add i64 %or.i350, %xor32
  %or.i352 = tail call i64 @llvm.fshl.i64(i64 %xor32, i64 %xor32, i64 21) #5
  %add36 = add i64 %xor28, %add30
  %or.i353 = tail call i64 @llvm.fshl.i64(i64 %xor28, i64 %xor28, i64 17) #5
  %xor38 = xor i64 %or.i353, %add36
  %or.i354 = tail call i64 @llvm.fshl.i64(i64 %add36, i64 %add36, i64 32) #5
  %xor42 = xor i64 %add33, %or
  %conv43 = zext i32 %third to i64
  %or44 = or i64 %conv43, 864691128455135232
  %xor35 = xor i64 %add33, %or44
  %xor45 = xor i64 %xor35, %or.i352
  %add47 = add i64 %xor42, %xor38
  %or.i355 = tail call i64 @llvm.fshl.i64(i64 %xor38, i64 %xor38, i64 13) #5
  %xor49 = xor i64 %or.i355, %add47
  %or.i356 = tail call i64 @llvm.fshl.i64(i64 %add47, i64 %add47, i64 32) #5
  %add51 = add i64 %xor45, %or.i354
  %or.i357 = tail call i64 @llvm.fshl.i64(i64 %xor45, i64 %xor45, i64 16) #5
  %xor53 = xor i64 %or.i357, %add51
  %add54 = add i64 %xor53, %or.i356
  %or.i358 = tail call i64 @llvm.fshl.i64(i64 %xor53, i64 %xor53, i64 21) #5
  %xor56 = xor i64 %or.i358, %add54
  %add57 = add i64 %add51, %xor49
  %or.i359 = tail call i64 @llvm.fshl.i64(i64 %xor49, i64 %xor49, i64 17) #5
  %xor59 = xor i64 %or.i359, %add57
  %or.i360 = tail call i64 @llvm.fshl.i64(i64 %add57, i64 %add57, i64 32) #5
  %add64 = add i64 %add54, %xor59
  %or.i361 = tail call i64 @llvm.fshl.i64(i64 %xor59, i64 %xor59, i64 13) #5
  %xor66 = xor i64 %or.i361, %add64
  %or.i362 = tail call i64 @llvm.fshl.i64(i64 %add64, i64 %add64, i64 32) #5
  %add68 = add i64 %xor56, %or.i360
  %or.i363 = tail call i64 @llvm.fshl.i64(i64 %xor56, i64 %xor56, i64 16) #5
  %xor70 = xor i64 %or.i363, %add68
  %add71 = add i64 %xor70, %or.i362
  %or.i364 = tail call i64 @llvm.fshl.i64(i64 %xor70, i64 %xor70, i64 21) #5
  %xor73 = xor i64 %or.i364, %add71
  %add74 = add i64 %add68, %xor66
  %or.i365 = tail call i64 @llvm.fshl.i64(i64 %xor66, i64 %xor66, i64 17) #5
  %xor76 = xor i64 %or.i365, %add74
  %or.i366 = tail call i64 @llvm.fshl.i64(i64 %add74, i64 %add74, i64 32) #5
  %xor80 = xor i64 %add71, %or44
  %xor81 = xor i64 %or.i366, 255
  %add83 = add i64 %xor80, %xor76
  %or.i367 = tail call i64 @llvm.fshl.i64(i64 %xor76, i64 %xor76, i64 13) #5
  %xor85 = xor i64 %add83, %or.i367
  %or.i368 = tail call i64 @llvm.fshl.i64(i64 %add83, i64 %add83, i64 32) #5
  %add87 = add i64 %xor81, %xor73
  %or.i369 = tail call i64 @llvm.fshl.i64(i64 %xor73, i64 %xor73, i64 16) #5
  %xor89 = xor i64 %or.i369, %add87
  %add90 = add i64 %or.i368, %xor89
  %or.i370 = tail call i64 @llvm.fshl.i64(i64 %xor89, i64 %xor89, i64 21) #5
  %xor92 = xor i64 %or.i370, %add90
  %add93 = add i64 %xor85, %add87
  %or.i371 = tail call i64 @llvm.fshl.i64(i64 %xor85, i64 %xor85, i64 17) #5
  %xor95 = xor i64 %or.i371, %add93
  %or.i372 = tail call i64 @llvm.fshl.i64(i64 %add93, i64 %add93, i64 32) #5
  %add100 = add i64 %xor95, %add90
  %or.i373 = tail call i64 @llvm.fshl.i64(i64 %xor95, i64 %xor95, i64 13) #5
  %xor102 = xor i64 %or.i373, %add100
  %or.i374 = tail call i64 @llvm.fshl.i64(i64 %add100, i64 %add100, i64 32) #5
  %add104 = add i64 %or.i372, %xor92
  %or.i375 = tail call i64 @llvm.fshl.i64(i64 %xor92, i64 %xor92, i64 16) #5
  %xor106 = xor i64 %or.i375, %add104
  %add107 = add i64 %or.i374, %xor106
  %or.i376 = tail call i64 @llvm.fshl.i64(i64 %xor106, i64 %xor106, i64 21) #5
  %xor109 = xor i64 %or.i376, %add107
  %add110 = add i64 %xor102, %add104
  %or.i377 = tail call i64 @llvm.fshl.i64(i64 %xor102, i64 %xor102, i64 17) #5
  %xor112 = xor i64 %or.i377, %add110
  %or.i378 = tail call i64 @llvm.fshl.i64(i64 %add110, i64 %add110, i64 32) #5
  %add117 = add i64 %xor112, %add107
  %or.i379 = tail call i64 @llvm.fshl.i64(i64 %xor112, i64 %xor112, i64 13) #5
  %xor119 = xor i64 %or.i379, %add117
  %or.i380 = tail call i64 @llvm.fshl.i64(i64 %add117, i64 %add117, i64 32) #5
  %add121 = add i64 %or.i378, %xor109
  %or.i381 = tail call i64 @llvm.fshl.i64(i64 %xor109, i64 %xor109, i64 16) #5
  %xor123 = xor i64 %or.i381, %add121
  %add124 = add i64 %or.i380, %xor123
  %or.i382 = tail call i64 @llvm.fshl.i64(i64 %xor123, i64 %xor123, i64 21) #5
  %xor126 = xor i64 %or.i382, %add124
  %add127 = add i64 %xor119, %add121
  %or.i383 = tail call i64 @llvm.fshl.i64(i64 %xor119, i64 %xor119, i64 17) #5
  %xor129 = xor i64 %or.i383, %add127
  %or.i384 = tail call i64 @llvm.fshl.i64(i64 %add127, i64 %add127, i64 32) #5
  %add134 = add i64 %xor129, %add124
  %or.i385 = tail call i64 @llvm.fshl.i64(i64 %xor129, i64 %xor129, i64 13) #5
  %xor136 = xor i64 %or.i385, %add134
  %add138 = add i64 %or.i384, %xor126
  %or.i387 = tail call i64 @llvm.fshl.i64(i64 %xor126, i64 %xor126, i64 16) #5
  %xor140 = xor i64 %or.i387, %add138
  %or.i388 = tail call i64 @llvm.fshl.i64(i64 %xor140, i64 %xor140, i64 21) #5
  %add144 = add i64 %xor136, %add138
  %or.i389 = tail call i64 @llvm.fshl.i64(i64 %xor136, i64 %xor136, i64 17) #5
  %xor146 = xor i64 %or.i389, %add144
  %or.i390 = tail call i64 @llvm.fshl.i64(i64 %add144, i64 %add144, i64 32) #5
  %xor150 = xor i64 %xor146, %or.i388
  %xor152 = xor i64 %xor150, %or.i390
  ret i64 %xor152
}

; Function Attrs: nofree nosync nounwind null_pointer_is_valid readonly sanitize_address sspstrong uwtable(sync)
define dso_local i32 @__hsiphash_unaligned(ptr noundef readonly %data, i32 noundef %len, ptr nocapture noundef readonly %key) #0 align 64 {
entry:
  call void @__sanitizer_cov_trace_pc() #4
  %add.ptr = getelementptr i8, ptr %data, i32 %len
  %rem = and i32 %len, 3
  %idx.neg = sub nsw i32 0, %rem
  %add.ptr1 = getelementptr i8, ptr %add.ptr, i32 %idx.neg
  %shl = shl i32 %len, 24
  %arrayidx = getelementptr [2 x i32], ptr %key, i32 0, i32 1
  %0 = ptrtoint ptr %arrayidx to i32
  call void @__asan_load4_noabort(i32 %0)
  %1 = load i32, ptr %arrayidx, align 4
  %xor = xor i32 %1, 1952801890
  %2 = ptrtoint ptr %key to i32
  call void @__asan_load4_noabort(i32 %2)
  %3 = load i32, ptr %key, align 4
  %xor5 = xor i32 %3, 1819895653
  %cmp.not276 = icmp eq ptr %add.ptr1, %data
  br i1 %cmp.not276, label %entry.for.end_crit_edge, label %entry.for.body_crit_edge

entry.for.body_crit_edge:                         ; preds = %entry
  br label %for.body

entry.for.end_crit_edge:                          ; preds = %entry
  call void @__sanitizer_cov_trace_pc() #4
  br label %for.end

for.body:                                         ; preds = %for.body.for.body_crit_edge, %entry.for.body_crit_edge
  %v3.0281 = phi i32 [ %xor22, %for.body.for.body_crit_edge ], [ %xor, %entry.for.body_crit_edge ]
  %v2.0280 = phi i32 [ %or.i251, %for.body.for.body_crit_edge ], [ %xor5, %entry.for.body_crit_edge ]
  %v1.0279 = phi i32 [ %xor25, %for.body.for.body_crit_edge ], [ %1, %entry.for.body_crit_edge ]
  %v0.0278 = phi i32 [ %xor27, %for.body.for.body_crit_edge ], [ %3, %entry.for.body_crit_edge ]
  %data.addr.0277 = phi ptr [ %add.ptr28, %for.body.for.body_crit_edge ], [ %data, %entry.for.body_crit_edge ]
  %4 = ptrtoint ptr %data.addr.0277 to i32
  call void @__asan_loadN_noabort(i32 %4, i32 4)
  %5 = load i32, ptr %data.addr.0277, align 1
  %6 = tail call i32 @llvm.bswap.i32(i32 %5) #5
  %xor13 = xor i32 %6, %v3.0281
  %add = add i32 %v1.0279, %v0.0278
  %or.i = tail call i32 @llvm.fshl.i32(i32 %v1.0279, i32 %v1.0279, i32 5) #5
  %xor15 = xor i32 %or.i, %add
  %or.i247 = tail call i32 @llvm.fshl.i32(i32 %add, i32 %add, i32 16) #5
  %add17 = add i32 %xor13, %v2.0280
  %or.i248 = tail call i32 @llvm.fshl.i32(i32 %xor13, i32 %xor13, i32 8) #5
  %xor19 = xor i32 %or.i248, %add17
  %add20 = add i32 %xor19, %or.i247
  %or.i249 = tail call i32 @llvm.fshl.i32(i32 %xor19, i32 %xor19, i32 7) #5
  %xor22 = xor i32 %or.i249, %add20
  %add23 = add i32 %add17, %xor15
  %or.i250 = tail call i32 @llvm.fshl.i32(i32 %xor15, i32 %xor15, i32 13) #5
  %xor25 = xor i32 %add23, %or.i250
  %or.i251 = tail call i32 @llvm.fshl.i32(i32 %add23, i32 %add23, i32 16) #5
  %xor27 = xor i32 %add20, %6
  %add.ptr28 = getelementptr i8, ptr %data.addr.0277, i32 4
  %cmp.not = icmp eq ptr %add.ptr28, %add.ptr1
  br i1 %cmp.not, label %for.body.for.end_crit_edge, label %for.body.for.body_crit_edge

for.body.for.body_crit_edge:                      ; preds = %for.body
  call void @__sanitizer_cov_trace_pc() #4
  br label %for.body

for.body.for.end_crit_edge:                       ; preds = %for.body
  call void @__sanitizer_cov_trace_pc() #4
  br label %for.end

for.end:                                          ; preds = %for.body.for.end_crit_edge, %entry.for.end_crit_edge
  %v0.0.lcssa = phi i32 [ %3, %entry.for.end_crit_edge ], [ %xor27, %for.body.for.end_crit_edge ]
  %v1.0.lcssa = phi i32 [ %1, %entry.for.end_crit_edge ], [ %xor25, %for.body.for.end_crit_edge ]
  %v2.0.lcssa = phi i32 [ %xor5, %entry.for.end_crit_edge ], [ %or.i251, %for.body.for.end_crit_edge ]
  %v3.0.lcssa = phi i32 [ %xor, %entry.for.end_crit_edge ], [ %xor22, %for.body.for.end_crit_edge ]
  %7 = zext i32 %rem to i64
  call void @__sanitizer_cov_trace_switch(i64 %7, ptr @__sancov_gen_cov_switch_values.1)
  switch i32 %rem, label %for.end.sw.epilog_crit_edge [
    i32 3, label %sw.bb
    i32 2, label %for.end.sw.bb33_crit_edge
    i32 1, label %sw.bb37
  ]

for.end.sw.bb33_crit_edge:                        ; preds = %for.end
  call void @__sanitizer_cov_trace_pc() #4
  br label %sw.bb33

for.end.sw.epilog_crit_edge:                      ; preds = %for.end
  call void @__sanitizer_cov_trace_pc() #4
  br label %sw.epilog

sw.bb:                                            ; preds = %for.end
  call void @__sanitizer_cov_trace_pc() #4
  %arrayidx30 = getelementptr i8, ptr %add.ptr1, i32 2
  %8 = ptrtoint ptr %arrayidx30 to i32
  call void @__asan_load1_noabort(i32 %8)
  %9 = load i8, ptr %arrayidx30, align 1
  %conv31 = zext i8 %9 to i32
  %shl32 = shl nuw nsw i32 %conv31, 16
  %or = or i32 %shl32, %shl
  br label %sw.bb33

sw.bb33:                                          ; preds = %sw.bb, %for.end.sw.bb33_crit_edge
  %b.0 = phi i32 [ %shl, %for.end.sw.bb33_crit_edge ], [ %or, %sw.bb ]
  %10 = ptrtoint ptr %add.ptr1 to i32
  call void @__asan_loadN_noabort(i32 %10, i32 2)
  %11 = load i16, ptr %add.ptr1, align 1
  %12 = tail call i16 @llvm.bswap.i16(i16 %11) #5
  %conv35 = zext i16 %12 to i32
  %or36 = or i32 %b.0, %conv35
  br label %sw.epilog

sw.bb37:                                          ; preds = %for.end
  call void @__sanitizer_cov_trace_pc() #4
  %13 = ptrtoint ptr %add.ptr1 to i32
  call void @__asan_load1_noabort(i32 %13)
  %14 = load i8, ptr %add.ptr1, align 1
  %conv39 = zext i8 %14 to i32
  %or40 = or i32 %shl, %conv39
  br label %sw.epilog

sw.epilog:                                        ; preds = %sw.bb37, %sw.bb33, %for.end.sw.epilog_crit_edge
  %b.1 = phi i32 [ %shl, %for.end.sw.epilog_crit_edge ], [ %or40, %sw.bb37 ], [ %or36, %sw.bb33 ]
  %xor41 = xor i32 %b.1, %v3.0.lcssa
  %add43 = add i32 %v1.0.lcssa, %v0.0.lcssa
  %or.i252 = tail call i32 @llvm.fshl.i32(i32 %v1.0.lcssa, i32 %v1.0.lcssa, i32 5) #5
  %xor45 = xor i32 %or.i252, %add43
  %or.i253 = tail call i32 @llvm.fshl.i32(i32 %add43, i32 %add43, i32 16) #5
  %add47 = add i32 %xor41, %v2.0.lcssa
  %or.i254 = tail call i32 @llvm.fshl.i32(i32 %xor41, i32 %xor41, i32 8) #5
  %xor49 = xor i32 %or.i254, %add47
  %add50 = add i32 %xor49, %or.i253
  %or.i255 = tail call i32 @llvm.fshl.i32(i32 %xor49, i32 %xor49, i32 7) #5
  %xor52 = xor i32 %or.i255, %add50
  %add53 = add i32 %add47, %xor45
  %or.i256 = tail call i32 @llvm.fshl.i32(i32 %xor45, i32 %xor45, i32 13) #5
  %xor55 = xor i32 %add53, %or.i256
  %or.i257 = tail call i32 @llvm.fshl.i32(i32 %add53, i32 %add53, i32 16) #5
  %xor59 = xor i32 %add50, %b.1
  %xor60 = xor i32 %or.i257, 255
  %add62 = add i32 %xor59, %xor55
  %or.i258 = tail call i32 @llvm.fshl.i32(i32 %xor55, i32 %xor55, i32 5) #5
  %xor64 = xor i32 %add62, %or.i258
  %or.i259 = tail call i32 @llvm.fshl.i32(i32 %add62, i32 %add62, i32 16) #5
  %add66 = add i32 %xor60, %xor52
  %or.i260 = tail call i32 @llvm.fshl.i32(i32 %xor52, i32 %xor52, i32 8) #5
  %xor68 = xor i32 %or.i260, %add66
  %add69 = add i32 %or.i259, %xor68
  %or.i261 = tail call i32 @llvm.fshl.i32(i32 %xor68, i32 %xor68, i32 7) #5
  %xor71 = xor i32 %or.i261, %add69
  %add72 = add i32 %xor64, %add66
  %or.i262 = tail call i32 @llvm.fshl.i32(i32 %xor64, i32 %xor64, i32 13) #5
  %xor74 = xor i32 %or.i262, %add72
  %or.i263 = tail call i32 @llvm.fshl.i32(i32 %add72, i32 %add72, i32 16) #5
  %add79 = add i32 %xor74, %add69
  %or.i264 = tail call i32 @llvm.fshl.i32(i32 %xor74, i32 %xor74, i32 5) #5
  %xor81 = xor i32 %or.i264, %add79
  %or.i265 = tail call i32 @llvm.fshl.i32(i32 %add79, i32 %add79, i32 16) #5
  %add83 = add i32 %or.i263, %xor71
  %or.i266 = tail call i32 @llvm.fshl.i32(i32 %xor71, i32 %xor71, i32 8) #5
  %xor85 = xor i32 %or.i266, %add83
  %add86 = add i32 %or.i265, %xor85
  %or.i267 = tail call i32 @llvm.fshl.i32(i32 %xor85, i32 %xor85, i32 7) #5
  %xor88 = xor i32 %or.i267, %add86
  %add89 = add i32 %xor81, %add83
  %or.i268 = tail call i32 @llvm.fshl.i32(i32 %xor81, i32 %xor81, i32 13) #5
  %xor91 = xor i32 %or.i268, %add89
  %or.i269 = tail call i32 @llvm.fshl.i32(i32 %add89, i32 %add89, i32 16) #5
  %add96 = add i32 %xor91, %add86
  %or.i270 = tail call i32 @llvm.fshl.i32(i32 %xor91, i32 %xor91, i32 5) #5
  %xor98 = xor i32 %or.i270, %add96
  %or.i271 = tail call i32 @llvm.fshl.i32(i32 %add96, i32 %add96, i32 16) #5
  %add100 = add i32 %or.i269, %xor88
  %or.i272 = tail call i32 @llvm.fshl.i32(i32 %xor88, i32 %xor88, i32 8) #5
  %xor102 = xor i32 %or.i272, %add100
  %add103 = add i32 %or.i271, %xor102
  %or.i273 = tail call i32 @llvm.fshl.i32(i32 %xor102, i32 %xor102, i32 7) #5
  %add106 = add i32 %xor98, %add100
  %or.i274 = tail call i32 @llvm.fshl.i32(i32 %xor98, i32 %xor98, i32 13) #5
  %xor105 = xor i32 %or.i274, %add106
  %xor108 = xor i32 %xor105, %add103
  %xor112 = xor i32 %xor108, %or.i273
  ret i32 %xor112
}

; Function Attrs: argmemonly mustprogress nofree nosync nounwind null_pointer_is_valid readonly sanitize_address sspstrong willreturn uwtable(sync)
define dso_local i32 @hsiphash_1u32(i32 noundef %first, ptr nocapture noundef readonly %key) #1 align 64 {
entry:
  call void @__sanitizer_cov_trace_pc() #4
  %arrayidx = getelementptr [2 x i32], ptr %key, i32 0, i32 1
  %0 = ptrtoint ptr %arrayidx to i32
  call void @__asan_load4_noabort(i32 %0)
  %1 = load i32, ptr %arrayidx, align 4
  %2 = ptrtoint ptr %key to i32
  call void @__asan_load4_noabort(i32 %2)
  %3 = load i32, ptr %key, align 4
  %xor4 = xor i32 %3, 1819895653
  %4 = xor i32 %1, %first
  %xor11 = xor i32 %4, 1952801890
  %add = add i32 %3, %1
  %or.i = tail call i32 @llvm.fshl.i32(i32 %1, i32 %1, i32 5) #5
  %xor12 = xor i32 %add, %or.i
  %or.i219 = tail call i32 @llvm.fshl.i32(i32 %add, i32 %add, i32 16) #5
  %add14 = add i32 %xor4, %xor11
  %or.i220 = tail call i32 @llvm.fshl.i32(i32 %xor11, i32 %xor11, i32 8) #5
  %xor16 = xor i32 %add14, %or.i220
  %add17 = add i32 %xor16, %or.i219
  %or.i221 = tail call i32 @llvm.fshl.i32(i32 %xor16, i32 %xor16, i32 7) #5
  %xor19 = xor i32 %or.i221, %add17
  %add20 = add i32 %add14, %xor12
  %or.i222 = tail call i32 @llvm.fshl.i32(i32 %xor12, i32 %xor12, i32 13) #5
  %xor22 = xor i32 %or.i222, %add20
  %or.i223 = tail call i32 @llvm.fshl.i32(i32 %add20, i32 %add20, i32 16) #5
  %xor24 = xor i32 %add17, %first
  %xor25 = xor i32 %xor19, 67108864
  %add27 = add i32 %xor24, %xor22
  %or.i224 = tail call i32 @llvm.fshl.i32(i32 %xor22, i32 %xor22, i32 5) #5
  %xor29 = xor i32 %add27, %or.i224
  %or.i225 = tail call i32 @llvm.fshl.i32(i32 %add27, i32 %add27, i32 16) #5
  %add31 = add i32 %xor25, %or.i223
  %or.i226 = tail call i32 @llvm.fshl.i32(i32 %xor19, i32 %xor25, i32 8) #5
  %xor33 = xor i32 %or.i226, %add31
  %add34 = add i32 %xor33, %or.i225
  %or.i227 = tail call i32 @llvm.fshl.i32(i32 %xor33, i32 %xor33, i32 7) #5
  %xor36 = xor i32 %or.i227, %add34
  %add37 = add i32 %add31, %xor29
  %or.i228 = tail call i32 @llvm.fshl.i32(i32 %xor29, i32 %xor29, i32 13) #5
  %xor39 = xor i32 %or.i228, %add37
  %or.i229 = tail call i32 @llvm.fshl.i32(i32 %add37, i32 %add37, i32 16) #5
  %xor43 = xor i32 %add34, 67108864
  %xor44 = xor i32 %or.i229, 255
  %add46 = add i32 %xor43, %xor39
  %or.i230 = tail call i32 @llvm.fshl.i32(i32 %xor39, i32 %xor39, i32 5) #5
  %xor48 = xor i32 %add46, %or.i230
  %or.i231 = tail call i32 @llvm.fshl.i32(i32 %add46, i32 %add46, i32 16) #5
  %add50 = add i32 %xor44, %xor36
  %or.i232 = tail call i32 @llvm.fshl.i32(i32 %xor36, i32 %xor36, i32 8) #5
  %xor52 = xor i32 %or.i232, %add50
  %add53 = add i32 %or.i231, %xor52
  %or.i233 = tail call i32 @llvm.fshl.i32(i32 %xor52, i32 %xor52, i32 7) #5
  %xor55 = xor i32 %or.i233, %add53
  %add56 = add i32 %xor48, %add50
  %or.i234 = tail call i32 @llvm.fshl.i32(i32 %xor48, i32 %xor48, i32 13) #5
  %xor58 = xor i32 %or.i234, %add56
  %or.i235 = tail call i32 @llvm.fshl.i32(i32 %add56, i32 %add56, i32 16) #5
  %add63 = add i32 %xor58, %add53
  %or.i236 = tail call i32 @llvm.fshl.i32(i32 %xor58, i32 %xor58, i32 5) #5
  %xor65 = xor i32 %or.i236, %add63
  %or.i237 = tail call i32 @llvm.fshl.i32(i32 %add63, i32 %add63, i32 16) #5
  %add67 = add i32 %or.i235, %xor55
  %or.i238 = tail call i32 @llvm.fshl.i32(i32 %xor55, i32 %xor55, i32 8) #5
  %xor69 = xor i32 %or.i238, %add67
  %add70 = add i32 %or.i237, %xor69
  %or.i239 = tail call i32 @llvm.fshl.i32(i32 %xor69, i32 %xor69, i32 7) #5
  %xor72 = xor i32 %or.i239, %add70
  %add73 = add i32 %xor65, %add67
  %or.i240 = tail call i32 @llvm.fshl.i32(i32 %xor65, i32 %xor65, i32 13) #5
  %xor75 = xor i32 %or.i240, %add73
  %or.i241 = tail call i32 @llvm.fshl.i32(i32 %add73, i32 %add73, i32 16) #5
  %add80 = add i32 %xor75, %add70
  %or.i242 = tail call i32 @llvm.fshl.i32(i32 %xor75, i32 %xor75, i32 5) #5
  %xor82 = xor i32 %or.i242, %add80
  %or.i243 = tail call i32 @llvm.fshl.i32(i32 %add80, i32 %add80, i32 16) #5
  %add84 = add i32 %or.i241, %xor72
  %or.i244 = tail call i32 @llvm.fshl.i32(i32 %xor72, i32 %xor72, i32 8) #5
  %xor86 = xor i32 %or.i244, %add84
  %add87 = add i32 %or.i243, %xor86
  %or.i245 = tail call i32 @llvm.fshl.i32(i32 %xor86, i32 %xor86, i32 7) #5
  %add90 = add i32 %xor82, %add84
  %or.i246 = tail call i32 @llvm.fshl.i32(i32 %xor82, i32 %xor82, i32 13) #5
  %xor89 = xor i32 %or.i246, %add90
  %xor92 = xor i32 %xor89, %add87
  %xor96 = xor i32 %xor92, %or.i245
  ret i32 %xor96
}

; Function Attrs: argmemonly mustprogress nofree nosync nounwind null_pointer_is_valid readonly sanitize_address sspstrong willreturn uwtable(sync)
define dso_local i32 @hsiphash_2u32(i32 noundef %first, i32 noundef %second, ptr nocapture noundef readonly %key) #1 align 64 {
entry:
  call void @__sanitizer_cov_trace_pc() #4
  %arrayidx = getelementptr [2 x i32], ptr %key, i32 0, i32 1
  %0 = ptrtoint ptr %arrayidx to i32
  call void @__asan_load4_noabort(i32 %0)
  %1 = load i32, ptr %arrayidx, align 4
  %2 = ptrtoint ptr %key to i32
  call void @__asan_load4_noabort(i32 %2)
  %3 = load i32, ptr %key, align 4
  %xor4 = xor i32 %3, 1819895653
  %4 = xor i32 %1, %first
  %xor11 = xor i32 %4, 1952801890
  %add = add i32 %3, %1
  %or.i = tail call i32 @llvm.fshl.i32(i32 %1, i32 %1, i32 5) #5
  %xor12 = xor i32 %add, %or.i
  %or.i263 = tail call i32 @llvm.fshl.i32(i32 %add, i32 %add, i32 16) #5
  %add14 = add i32 %xor4, %xor11
  %or.i264 = tail call i32 @llvm.fshl.i32(i32 %xor11, i32 %xor11, i32 8) #5
  %xor16 = xor i32 %add14, %or.i264
  %add17 = add i32 %xor16, %or.i263
  %or.i265 = tail call i32 @llvm.fshl.i32(i32 %xor16, i32 %xor16, i32 7) #5
  %add20 = add i32 %add14, %xor12
  %or.i266 = tail call i32 @llvm.fshl.i32(i32 %xor12, i32 %xor12, i32 13) #5
  %xor22 = xor i32 %or.i266, %add20
  %or.i267 = tail call i32 @llvm.fshl.i32(i32 %add20, i32 %add20, i32 16) #5
  %xor24 = xor i32 %add17, %first
  %xor19 = xor i32 %add17, %second
  %xor25 = xor i32 %xor19, %or.i265
  %add27 = add i32 %xor24, %xor22
  %or.i268 = tail call i32 @llvm.fshl.i32(i32 %xor22, i32 %xor22, i32 5) #5
  %xor29 = xor i32 %add27, %or.i268
  %or.i269 = tail call i32 @llvm.fshl.i32(i32 %add27, i32 %add27, i32 16) #5
  %add31 = add i32 %xor25, %or.i267
  %or.i270 = tail call i32 @llvm.fshl.i32(i32 %xor25, i32 %xor25, i32 8) #5
  %xor33 = xor i32 %or.i270, %add31
  %add34 = add i32 %xor33, %or.i269
  %or.i271 = tail call i32 @llvm.fshl.i32(i32 %xor33, i32 %xor33, i32 7) #5
  %xor36 = xor i32 %or.i271, %add34
  %add37 = add i32 %add31, %xor29
  %or.i272 = tail call i32 @llvm.fshl.i32(i32 %xor29, i32 %xor29, i32 13) #5
  %xor39 = xor i32 %or.i272, %add37
  %or.i273 = tail call i32 @llvm.fshl.i32(i32 %add37, i32 %add37, i32 16) #5
  %xor43 = xor i32 %add34, %second
  %xor44 = xor i32 %xor36, 134217728
  %add46 = add i32 %xor43, %xor39
  %or.i274 = tail call i32 @llvm.fshl.i32(i32 %xor39, i32 %xor39, i32 5) #5
  %xor48 = xor i32 %add46, %or.i274
  %or.i275 = tail call i32 @llvm.fshl.i32(i32 %add46, i32 %add46, i32 16) #5
  %add50 = add i32 %xor44, %or.i273
  %or.i276 = tail call i32 @llvm.fshl.i32(i32 %xor36, i32 %xor44, i32 8) #5
  %xor52 = xor i32 %or.i276, %add50
  %add53 = add i32 %xor52, %or.i275
  %or.i277 = tail call i32 @llvm.fshl.i32(i32 %xor52, i32 %xor52, i32 7) #5
  %xor55 = xor i32 %or.i277, %add53
  %add56 = add i32 %add50, %xor48
  %or.i278 = tail call i32 @llvm.fshl.i32(i32 %xor48, i32 %xor48, i32 13) #5
  %xor58 = xor i32 %or.i278, %add56
  %or.i279 = tail call i32 @llvm.fshl.i32(i32 %add56, i32 %add56, i32 16) #5
  %xor62 = xor i32 %add53, 134217728
  %xor63 = xor i32 %or.i279, 255
  %add65 = add i32 %xor62, %xor58
  %or.i280 = tail call i32 @llvm.fshl.i32(i32 %xor58, i32 %xor58, i32 5) #5
  %xor67 = xor i32 %add65, %or.i280
  %or.i281 = tail call i32 @llvm.fshl.i32(i32 %add65, i32 %add65, i32 16) #5
  %add69 = add i32 %xor63, %xor55
  %or.i282 = tail call i32 @llvm.fshl.i32(i32 %xor55, i32 %xor55, i32 8) #5
  %xor71 = xor i32 %or.i282, %add69
  %add72 = add i32 %or.i281, %xor71
  %or.i283 = tail call i32 @llvm.fshl.i32(i32 %xor71, i32 %xor71, i32 7) #5
  %xor74 = xor i32 %or.i283, %add72
  %add75 = add i32 %xor67, %add69
  %or.i284 = tail call i32 @llvm.fshl.i32(i32 %xor67, i32 %xor67, i32 13) #5
  %xor77 = xor i32 %or.i284, %add75
  %or.i285 = tail call i32 @llvm.fshl.i32(i32 %add75, i32 %add75, i32 16) #5
  %add82 = add i32 %xor77, %add72
  %or.i286 = tail call i32 @llvm.fshl.i32(i32 %xor77, i32 %xor77, i32 5) #5
  %xor84 = xor i32 %or.i286, %add82
  %or.i287 = tail call i32 @llvm.fshl.i32(i32 %add82, i32 %add82, i32 16) #5
  %add86 = add i32 %or.i285, %xor74
  %or.i288 = tail call i32 @llvm.fshl.i32(i32 %xor74, i32 %xor74, i32 8) #5
  %xor88 = xor i32 %or.i288, %add86
  %add89 = add i32 %or.i287, %xor88
  %or.i289 = tail call i32 @llvm.fshl.i32(i32 %xor88, i32 %xor88, i32 7) #5
  %xor91 = xor i32 %or.i289, %add89
  %add92 = add i32 %xor84, %add86
  %or.i290 = tail call i32 @llvm.fshl.i32(i32 %xor84, i32 %xor84, i32 13) #5
  %xor94 = xor i32 %or.i290, %add92
  %or.i291 = tail call i32 @llvm.fshl.i32(i32 %add92, i32 %add92, i32 16) #5
  %add99 = add i32 %xor94, %add89
  %or.i292 = tail call i32 @llvm.fshl.i32(i32 %xor94, i32 %xor94, i32 5) #5
  %xor101 = xor i32 %or.i292, %add99
  %or.i293 = tail call i32 @llvm.fshl.i32(i32 %add99, i32 %add99, i32 16) #5
  %add103 = add i32 %or.i291, %xor91
  %or.i294 = tail call i32 @llvm.fshl.i32(i32 %xor91, i32 %xor91, i32 8) #5
  %xor105 = xor i32 %or.i294, %add103
  %add106 = add i32 %or.i293, %xor105
  %or.i295 = tail call i32 @llvm.fshl.i32(i32 %xor105, i32 %xor105, i32 7) #5
  %add109 = add i32 %xor101, %add103
  %or.i296 = tail call i32 @llvm.fshl.i32(i32 %xor101, i32 %xor101, i32 13) #5
  %xor108 = xor i32 %or.i296, %add109
  %xor111 = xor i32 %xor108, %add106
  %xor115 = xor i32 %xor111, %or.i295
  ret i32 %xor115
}

; Function Attrs: argmemonly mustprogress nofree nosync nounwind null_pointer_is_valid readonly sanitize_address sspstrong willreturn uwtable(sync)
define dso_local i32 @hsiphash_3u32(i32 noundef %first, i32 noundef %second, i32 noundef %third, ptr nocapture noundef readonly %key) #1 align 64 {
entry:
  call void @__sanitizer_cov_trace_pc() #4
  %arrayidx = getelementptr [2 x i32], ptr %key, i32 0, i32 1
  %0 = ptrtoint ptr %arrayidx to i32
  call void @__asan_load4_noabort(i32 %0)
  %1 = load i32, ptr %arrayidx, align 4
  %2 = ptrtoint ptr %key to i32
  call void @__asan_load4_noabort(i32 %2)
  %3 = load i32, ptr %key, align 4
  %xor4 = xor i32 %3, 1819895653
  %4 = xor i32 %1, %first
  %xor11 = xor i32 %4, 1952801890
  %add = add i32 %3, %1
  %or.i = tail call i32 @llvm.fshl.i32(i32 %1, i32 %1, i32 5) #5
  %xor12 = xor i32 %add, %or.i
  %or.i307 = tail call i32 @llvm.fshl.i32(i32 %add, i32 %add, i32 16) #5
  %add14 = add i32 %xor4, %xor11
  %or.i308 = tail call i32 @llvm.fshl.i32(i32 %xor11, i32 %xor11, i32 8) #5
  %xor16 = xor i32 %add14, %or.i308
  %add17 = add i32 %xor16, %or.i307
  %or.i309 = tail call i32 @llvm.fshl.i32(i32 %xor16, i32 %xor16, i32 7) #5
  %add20 = add i32 %add14, %xor12
  %or.i310 = tail call i32 @llvm.fshl.i32(i32 %xor12, i32 %xor12, i32 13) #5
  %xor22 = xor i32 %or.i310, %add20
  %or.i311 = tail call i32 @llvm.fshl.i32(i32 %add20, i32 %add20, i32 16) #5
  %xor24 = xor i32 %add17, %first
  %xor19 = xor i32 %add17, %second
  %xor25 = xor i32 %xor19, %or.i309
  %add27 = add i32 %xor24, %xor22
  %or.i312 = tail call i32 @llvm.fshl.i32(i32 %xor22, i32 %xor22, i32 5) #5
  %xor29 = xor i32 %add27, %or.i312
  %or.i313 = tail call i32 @llvm.fshl.i32(i32 %add27, i32 %add27, i32 16) #5
  %add31 = add i32 %xor25, %or.i311
  %or.i314 = tail call i32 @llvm.fshl.i32(i32 %xor25, i32 %xor25, i32 8) #5
  %xor33 = xor i32 %or.i314, %add31
  %add34 = add i32 %xor33, %or.i313
  %or.i315 = tail call i32 @llvm.fshl.i32(i32 %xor33, i32 %xor33, i32 7) #5
  %add37 = add i32 %add31, %xor29
  %or.i316 = tail call i32 @llvm.fshl.i32(i32 %xor29, i32 %xor29, i32 13) #5
  %xor39 = xor i32 %or.i316, %add37
  %or.i317 = tail call i32 @llvm.fshl.i32(i32 %add37, i32 %add37, i32 16) #5
  %xor43 = xor i32 %add34, %second
  %xor36 = xor i32 %add34, %third
  %xor44 = xor i32 %xor36, %or.i315
  %add46 = add i32 %xor43, %xor39
  %or.i318 = tail call i32 @llvm.fshl.i32(i32 %xor39, i32 %xor39, i32 5) #5
  %xor48 = xor i32 %add46, %or.i318
  %or.i319 = tail call i32 @llvm.fshl.i32(i32 %add46, i32 %add46, i32 16) #5
  %add50 = add i32 %xor44, %or.i317
  %or.i320 = tail call i32 @llvm.fshl.i32(i32 %xor44, i32 %xor44, i32 8) #5
  %xor52 = xor i32 %or.i320, %add50
  %add53 = add i32 %xor52, %or.i319
  %or.i321 = tail call i32 @llvm.fshl.i32(i32 %xor52, i32 %xor52, i32 7) #5
  %xor55 = xor i32 %or.i321, %add53
  %add56 = add i32 %add50, %xor48
  %or.i322 = tail call i32 @llvm.fshl.i32(i32 %xor48, i32 %xor48, i32 13) #5
  %xor58 = xor i32 %or.i322, %add56
  %or.i323 = tail call i32 @llvm.fshl.i32(i32 %add56, i32 %add56, i32 16) #5
  %xor62 = xor i32 %add53, %third
  %xor63 = xor i32 %xor55, 201326592
  %add65 = add i32 %xor62, %xor58
  %or.i324 = tail call i32 @llvm.fshl.i32(i32 %xor58, i32 %xor58, i32 5) #5
  %xor67 = xor i32 %add65, %or.i324
  %or.i325 = tail call i32 @llvm.fshl.i32(i32 %add65, i32 %add65, i32 16) #5
  %add69 = add i32 %xor63, %or.i323
  %or.i326 = tail call i32 @llvm.fshl.i32(i32 %xor55, i32 %xor63, i32 8) #5
  %xor71 = xor i32 %or.i326, %add69
  %add72 = add i32 %xor71, %or.i325
  %or.i327 = tail call i32 @llvm.fshl.i32(i32 %xor71, i32 %xor71, i32 7) #5
  %xor74 = xor i32 %or.i327, %add72
  %add75 = add i32 %add69, %xor67
  %or.i328 = tail call i32 @llvm.fshl.i32(i32 %xor67, i32 %xor67, i32 13) #5
  %xor77 = xor i32 %or.i328, %add75
  %or.i329 = tail call i32 @llvm.fshl.i32(i32 %add75, i32 %add75, i32 16) #5
  %xor81 = xor i32 %add72, 201326592
  %xor82 = xor i32 %or.i329, 255
  %add84 = add i32 %xor81, %xor77
  %or.i330 = tail call i32 @llvm.fshl.i32(i32 %xor77, i32 %xor77, i32 5) #5
  %xor86 = xor i32 %add84, %or.i330
  %or.i331 = tail call i32 @llvm.fshl.i32(i32 %add84, i32 %add84, i32 16) #5
  %add88 = add i32 %xor82, %xor74
  %or.i332 = tail call i32 @llvm.fshl.i32(i32 %xor74, i32 %xor74, i32 8) #5
  %xor90 = xor i32 %or.i332, %add88
  %add91 = add i32 %or.i331, %xor90
  %or.i333 = tail call i32 @llvm.fshl.i32(i32 %xor90, i32 %xor90, i32 7) #5
  %xor93 = xor i32 %or.i333, %add91
  %add94 = add i32 %xor86, %add88
  %or.i334 = tail call i32 @llvm.fshl.i32(i32 %xor86, i32 %xor86, i32 13) #5
  %xor96 = xor i32 %or.i334, %add94
  %or.i335 = tail call i32 @llvm.fshl.i32(i32 %add94, i32 %add94, i32 16) #5
  %add101 = add i32 %xor96, %add91
  %or.i336 = tail call i32 @llvm.fshl.i32(i32 %xor96, i32 %xor96, i32 5) #5
  %xor103 = xor i32 %or.i336, %add101
  %or.i337 = tail call i32 @llvm.fshl.i32(i32 %add101, i32 %add101, i32 16) #5
  %add105 = add i32 %or.i335, %xor93
  %or.i338 = tail call i32 @llvm.fshl.i32(i32 %xor93, i32 %xor93, i32 8) #5
  %xor107 = xor i32 %or.i338, %add105
  %add108 = add i32 %or.i337, %xor107
  %or.i339 = tail call i32 @llvm.fshl.i32(i32 %xor107, i32 %xor107, i32 7) #5
  %xor110 = xor i32 %or.i339, %add108
  %add111 = add i32 %xor103, %add105
  %or.i340 = tail call i32 @llvm.fshl.i32(i32 %xor103, i32 %xor103, i32 13) #5
  %xor113 = xor i32 %or.i340, %add111
  %or.i341 = tail call i32 @llvm.fshl.i32(i32 %add111, i32 %add111, i32 16) #5
  %add118 = add i32 %xor113, %add108
  %or.i342 = tail call i32 @llvm.fshl.i32(i32 %xor113, i32 %xor113, i32 5) #5
  %xor120 = xor i32 %or.i342, %add118
  %or.i343 = tail call i32 @llvm.fshl.i32(i32 %add118, i32 %add118, i32 16) #5
  %add122 = add i32 %or.i341, %xor110
  %or.i344 = tail call i32 @llvm.fshl.i32(i32 %xor110, i32 %xor110, i32 8) #5
  %xor124 = xor i32 %or.i344, %add122
  %add125 = add i32 %or.i343, %xor124
  %or.i345 = tail call i32 @llvm.fshl.i32(i32 %xor124, i32 %xor124, i32 7) #5
  %add128 = add i32 %xor120, %add122
  %or.i346 = tail call i32 @llvm.fshl.i32(i32 %xor120, i32 %xor120, i32 13) #5
  %xor127 = xor i32 %or.i346, %add128
  %xor130 = xor i32 %xor127, %add125
  %xor134 = xor i32 %xor130, %or.i345
  ret i32 %xor134
}

; Function Attrs: argmemonly mustprogress nofree nosync nounwind null_pointer_is_valid readonly sanitize_address sspstrong willreturn uwtable(sync)
define dso_local i32 @hsiphash_4u32(i32 noundef %first, i32 noundef %second, i32 noundef %third, i32 noundef %forth, ptr nocapture noundef readonly %key) #1 align 64 {
entry:
  call void @__sanitizer_cov_trace_pc() #4
  %arrayidx = getelementptr [2 x i32], ptr %key, i32 0, i32 1
  %0 = ptrtoint ptr %arrayidx to i32
  call void @__asan_load4_noabort(i32 %0)
  %1 = load i32, ptr %arrayidx, align 4
  %2 = ptrtoint ptr %key to i32
  call void @__asan_load4_noabort(i32 %2)
  %3 = load i32, ptr %key, align 4
  %xor4 = xor i32 %3, 1819895653
  %4 = xor i32 %1, %first
  %xor11 = xor i32 %4, 1952801890
  %add = add i32 %3, %1
  %or.i = tail call i32 @llvm.fshl.i32(i32 %1, i32 %1, i32 5) #5
  %xor12 = xor i32 %add, %or.i
  %or.i351 = tail call i32 @llvm.fshl.i32(i32 %add, i32 %add, i32 16) #5
  %add14 = add i32 %xor4, %xor11
  %or.i352 = tail call i32 @llvm.fshl.i32(i32 %xor11, i32 %xor11, i32 8) #5
  %xor16 = xor i32 %add14, %or.i352
  %add17 = add i32 %xor16, %or.i351
  %or.i353 = tail call i32 @llvm.fshl.i32(i32 %xor16, i32 %xor16, i32 7) #5
  %add20 = add i32 %add14, %xor12
  %or.i354 = tail call i32 @llvm.fshl.i32(i32 %xor12, i32 %xor12, i32 13) #5
  %xor22 = xor i32 %or.i354, %add20
  %or.i355 = tail call i32 @llvm.fshl.i32(i32 %add20, i32 %add20, i32 16) #5
  %xor24 = xor i32 %add17, %first
  %xor19 = xor i32 %add17, %second
  %xor25 = xor i32 %xor19, %or.i353
  %add27 = add i32 %xor24, %xor22
  %or.i356 = tail call i32 @llvm.fshl.i32(i32 %xor22, i32 %xor22, i32 5) #5
  %xor29 = xor i32 %add27, %or.i356
  %or.i357 = tail call i32 @llvm.fshl.i32(i32 %add27, i32 %add27, i32 16) #5
  %add31 = add i32 %xor25, %or.i355
  %or.i358 = tail call i32 @llvm.fshl.i32(i32 %xor25, i32 %xor25, i32 8) #5
  %xor33 = xor i32 %or.i358, %add31
  %add34 = add i32 %xor33, %or.i357
  %or.i359 = tail call i32 @llvm.fshl.i32(i32 %xor33, i32 %xor33, i32 7) #5
  %add37 = add i32 %add31, %xor29
  %or.i360 = tail call i32 @llvm.fshl.i32(i32 %xor29, i32 %xor29, i32 13) #5
  %xor39 = xor i32 %or.i360, %add37
  %or.i361 = tail call i32 @llvm.fshl.i32(i32 %add37, i32 %add37, i32 16) #5
  %xor43 = xor i32 %add34, %second
  %xor36 = xor i32 %add34, %third
  %xor44 = xor i32 %xor36, %or.i359
  %add46 = add i32 %xor43, %xor39
  %or.i362 = tail call i32 @llvm.fshl.i32(i32 %xor39, i32 %xor39, i32 5) #5
  %xor48 = xor i32 %add46, %or.i362
  %or.i363 = tail call i32 @llvm.fshl.i32(i32 %add46, i32 %add46, i32 16) #5
  %add50 = add i32 %xor44, %or.i361
  %or.i364 = tail call i32 @llvm.fshl.i32(i32 %xor44, i32 %xor44, i32 8) #5
  %xor52 = xor i32 %or.i364, %add50
  %add53 = add i32 %xor52, %or.i363
  %or.i365 = tail call i32 @llvm.fshl.i32(i32 %xor52, i32 %xor52, i32 7) #5
  %add56 = add i32 %add50, %xor48
  %or.i366 = tail call i32 @llvm.fshl.i32(i32 %xor48, i32 %xor48, i32 13) #5
  %xor58 = xor i32 %or.i366, %add56
  %or.i367 = tail call i32 @llvm.fshl.i32(i32 %add56, i32 %add56, i32 16) #5
  %xor62 = xor i32 %add53, %third
  %xor55 = xor i32 %add53, %forth
  %xor63 = xor i32 %xor55, %or.i365
  %add65 = add i32 %xor62, %xor58
  %or.i368 = tail call i32 @llvm.fshl.i32(i32 %xor58, i32 %xor58, i32 5) #5
  %xor67 = xor i32 %add65, %or.i368
  %or.i369 = tail call i32 @llvm.fshl.i32(i32 %add65, i32 %add65, i32 16) #5
  %add69 = add i32 %xor63, %or.i367
  %or.i370 = tail call i32 @llvm.fshl.i32(i32 %xor63, i32 %xor63, i32 8) #5
  %xor71 = xor i32 %or.i370, %add69
  %add72 = add i32 %xor71, %or.i369
  %or.i371 = tail call i32 @llvm.fshl.i32(i32 %xor71, i32 %xor71, i32 7) #5
  %xor74 = xor i32 %or.i371, %add72
  %add75 = add i32 %add69, %xor67
  %or.i372 = tail call i32 @llvm.fshl.i32(i32 %xor67, i32 %xor67, i32 13) #5
  %xor77 = xor i32 %or.i372, %add75
  %or.i373 = tail call i32 @llvm.fshl.i32(i32 %add75, i32 %add75, i32 16) #5
  %xor81 = xor i32 %add72, %forth
  %xor82 = xor i32 %xor74, 268435456
  %add84 = add i32 %xor81, %xor77
  %or.i374 = tail call i32 @llvm.fshl.i32(i32 %xor77, i32 %xor77, i32 5) #5
  %xor86 = xor i32 %add84, %or.i374
  %or.i375 = tail call i32 @llvm.fshl.i32(i32 %add84, i32 %add84, i32 16) #5
  %add88 = add i32 %xor82, %or.i373
  %or.i376 = tail call i32 @llvm.fshl.i32(i32 %xor74, i32 %xor82, i32 8) #5
  %xor90 = xor i32 %or.i376, %add88
  %add91 = add i32 %xor90, %or.i375
  %or.i377 = tail call i32 @llvm.fshl.i32(i32 %xor90, i32 %xor90, i32 7) #5
  %xor93 = xor i32 %or.i377, %add91
  %add94 = add i32 %add88, %xor86
  %or.i378 = tail call i32 @llvm.fshl.i32(i32 %xor86, i32 %xor86, i32 13) #5
  %xor96 = xor i32 %or.i378, %add94
  %or.i379 = tail call i32 @llvm.fshl.i32(i32 %add94, i32 %add94, i32 16) #5
  %xor100 = xor i32 %add91, 268435456
  %xor101 = xor i32 %or.i379, 255
  %add103 = add i32 %xor100, %xor96
  %or.i380 = tail call i32 @llvm.fshl.i32(i32 %xor96, i32 %xor96, i32 5) #5
  %xor105 = xor i32 %add103, %or.i380
  %or.i381 = tail call i32 @llvm.fshl.i32(i32 %add103, i32 %add103, i32 16) #5
  %add107 = add i32 %xor101, %xor93
  %or.i382 = tail call i32 @llvm.fshl.i32(i32 %xor93, i32 %xor93, i32 8) #5
  %xor109 = xor i32 %or.i382, %add107
  %add110 = add i32 %or.i381, %xor109
  %or.i383 = tail call i32 @llvm.fshl.i32(i32 %xor109, i32 %xor109, i32 7) #5
  %xor112 = xor i32 %or.i383, %add110
  %add113 = add i32 %xor105, %add107
  %or.i384 = tail call i32 @llvm.fshl.i32(i32 %xor105, i32 %xor105, i32 13) #5
  %xor115 = xor i32 %or.i384, %add113
  %or.i385 = tail call i32 @llvm.fshl.i32(i32 %add113, i32 %add113, i32 16) #5
  %add120 = add i32 %xor115, %add110
  %or.i386 = tail call i32 @llvm.fshl.i32(i32 %xor115, i32 %xor115, i32 5) #5
  %xor122 = xor i32 %or.i386, %add120
  %or.i387 = tail call i32 @llvm.fshl.i32(i32 %add120, i32 %add120, i32 16) #5
  %add124 = add i32 %or.i385, %xor112
  %or.i388 = tail call i32 @llvm.fshl.i32(i32 %xor112, i32 %xor112, i32 8) #5
  %xor126 = xor i32 %or.i388, %add124
  %add127 = add i32 %or.i387, %xor126
  %or.i389 = tail call i32 @llvm.fshl.i32(i32 %xor126, i32 %xor126, i32 7) #5
  %xor129 = xor i32 %or.i389, %add127
  %add130 = add i32 %xor122, %add124
  %or.i390 = tail call i32 @llvm.fshl.i32(i32 %xor122, i32 %xor122, i32 13) #5
  %xor132 = xor i32 %or.i390, %add130
  %or.i391 = tail call i32 @llvm.fshl.i32(i32 %add130, i32 %add130, i32 16) #5
  %add137 = add i32 %xor132, %add127
  %or.i392 = tail call i32 @llvm.fshl.i32(i32 %xor132, i32 %xor132, i32 5) #5
  %xor139 = xor i32 %or.i392, %add137
  %or.i393 = tail call i32 @llvm.fshl.i32(i32 %add137, i32 %add137, i32 16) #5
  %add141 = add i32 %or.i391, %xor129
  %or.i394 = tail call i32 @llvm.fshl.i32(i32 %xor129, i32 %xor129, i32 8) #5
  %xor143 = xor i32 %or.i394, %add141
  %add144 = add i32 %or.i393, %xor143
  %or.i395 = tail call i32 @llvm.fshl.i32(i32 %xor143, i32 %xor143, i32 7) #5
  %add147 = add i32 %xor139, %add141
  %or.i396 = tail call i32 @llvm.fshl.i32(i32 %xor139, i32 %xor139, i32 13) #5
  %xor146 = xor i32 %or.i396, %add147
  %xor149 = xor i32 %xor146, %add144
  %xor153 = xor i32 %xor149, %or.i395
  ret i32 %xor153
}

; Function Attrs: nocallback nofree nosync nounwind readnone speculatable willreturn
declare i64 @llvm.bswap.i64(i64) #2

; Function Attrs: nocallback nofree nosync nounwind readnone speculatable willreturn
declare i32 @llvm.bswap.i32(i32) #2

; Function Attrs: nocallback nofree nosync nounwind readnone speculatable willreturn
declare i16 @llvm.bswap.i16(i16) #2

; Function Attrs: nocallback nofree nosync nounwind readnone speculatable willreturn
declare i64 @llvm.fshl.i64(i64, i64, i64) #2

; Function Attrs: nocallback nofree nosync nounwind readnone speculatable willreturn
declare i32 @llvm.fshl.i32(i32, i32, i32) #2

declare void @__sanitizer_cov_trace_switch(i64, ptr)

declare void @__sanitizer_cov_trace_pc()

declare void @__asan_loadN_noabort(i32, i32)

declare void @__asan_load1_noabort(i32)

declare void @__asan_load4_noabort(i32)

declare void @__asan_load8_noabort(i32)

; Function Attrs: nounwind uwtable(sync)
define internal void @asan.module_ctor() #3 {
  ret void
}

attributes #0 = { nofree nosync nounwind null_pointer_is_valid readonly sanitize_address sspstrong uwtable(sync) "frame-pointer"="none" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="mpcore" "target-features"="+armv6k,+dsp,+soft-float,+strict-align,-aes,-bf16,-d32,-dotprod,-fp-armv8,-fp-armv8d16,-fp-armv8d16sp,-fp-armv8sp,-fp16,-fp16fml,-fp64,-fpregs,-fullfp16,-mve,-mve.fp,-neon,-sha2,-thumb-mode,-vfp2,-vfp2sp,-vfp3,-vfp3d16,-vfp3d16sp,-vfp3sp,-vfp4,-vfp4d16,-vfp4d16sp,-vfp4sp" "use-soft-float"="true" "warn-stack-size"="1024" }
attributes #1 = { argmemonly mustprogress nofree nosync nounwind null_pointer_is_valid readonly sanitize_address sspstrong willreturn uwtable(sync) "frame-pointer"="none" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="mpcore" "target-features"="+armv6k,+dsp,+soft-float,+strict-align,-aes,-bf16,-d32,-dotprod,-fp-armv8,-fp-armv8d16,-fp-armv8d16sp,-fp-armv8sp,-fp16,-fp16fml,-fp64,-fpregs,-fullfp16,-mve,-mve.fp,-neon,-sha2,-thumb-mode,-vfp2,-vfp2sp,-vfp3,-vfp3d16,-vfp3d16sp,-vfp3sp,-vfp4,-vfp4d16,-vfp4d16sp,-vfp4sp" "use-soft-float"="true" "warn-stack-size"="1024" }
attributes #2 = { nocallback nofree nosync nounwind readnone speculatable willreturn }
attributes #3 = { nounwind uwtable(sync) }
attributes #4 = { nomerge }
attributes #5 = { nounwind }

!llvm.asan.globals = !{!0, !2, !4, !6, !8, !10, !12, !14, !16, !18, !20, !22}
!llvm.module.flags = !{!24, !25, !26, !27, !28, !29, !30}
!llvm.ident = !{!31}

!0 = !{ptr @__ksymtab___siphash_unaligned, !1, !"__ksymtab___siphash_unaligned", i1 false, i1 false}
!1 = !{!"../lib/siphash.c", i32 116, i32 1}
!2 = !{ptr @__ksymtab_siphash_1u64, !3, !"__ksymtab_siphash_1u64", i1 false, i1 false}
!3 = !{!"../lib/siphash.c", i32 132, i32 1}
!4 = !{ptr @__ksymtab_siphash_2u64, !5, !"__ksymtab_siphash_2u64", i1 false, i1 false}
!5 = !{!"../lib/siphash.c", i32 153, i32 1}
!6 = !{ptr @__ksymtab_siphash_3u64, !7, !"__ksymtab_siphash_3u64", i1 false, i1 false}
!7 = !{!"../lib/siphash.c", i32 180, i32 1}
!8 = !{ptr @__ksymtab_siphash_4u64, !9, !"__ksymtab_siphash_4u64", i1 false, i1 false}
!9 = !{!"../lib/siphash.c", i32 212, i32 1}
!10 = !{ptr @__ksymtab_siphash_1u32, !11, !"__ksymtab_siphash_1u32", i1 false, i1 false}
!11 = !{!"../lib/siphash.c", i32 220, i32 1}
!12 = !{ptr @__ksymtab_siphash_3u32, !13, !"__ksymtab_siphash_3u32", i1 false, i1 false}
!13 = !{!"../lib/siphash.c", i32 234, i32 1}
!14 = !{ptr @__ksymtab___hsiphash_unaligned, !15, !"__ksymtab___hsiphash_unaligned", i1 false, i1 false}
!15 = !{!"../lib/siphash.c", i32 464, i32 1}
!16 = !{ptr @__ksymtab_hsiphash_1u32, !17, !"__ksymtab_hsiphash_1u32", i1 false, i1 false}
!17 = !{!"../lib/siphash.c", i32 479, i32 1}
!18 = !{ptr @__ksymtab_hsiphash_2u32, !19, !"__ksymtab_hsiphash_2u32", i1 false, i1 false}
!19 = !{!"../lib/siphash.c", i32 498, i32 1}
!20 = !{ptr @__ksymtab_hsiphash_3u32, !21, !"__ksymtab_hsiphash_3u32", i1 false, i1 false}
!21 = !{!"../lib/siphash.c", i32 522, i32 1}
!22 = !{ptr @__ksymtab_hsiphash_4u32, !23, !"__ksymtab_hsiphash_4u32", i1 false, i1 false}
!23 = !{!"../lib/siphash.c", i32 550, i32 1}
!24 = !{i32 1, !"wchar_size", i32 2}
!25 = !{i32 1, !"min_enum_size", i32 4}
!26 = !{i32 8, !"branch-target-enforcement", i32 0}
!27 = !{i32 8, !"sign-return-address", i32 0}
!28 = !{i32 8, !"sign-return-address-all", i32 0}
!29 = !{i32 8, !"sign-return-address-with-bkey", i32 0}
!30 = !{i32 7, !"uwtable", i32 1}
!31 = !{!"clang version 15.0.0 (git@github.com:linkeLi0421/llvm-project15-IRDumperPass.git 23ab625cb005cd08da083f9b643a7feed9af8abe)"}
